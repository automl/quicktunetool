{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Quick-Tune-Tool","text":"<p>A Practical Tool and User Guide for Automatically Finetuning Pretrained Models</p> <p>Quick-Tune-Tool is an automated solution for selecting and finetuning pretrained models across various machine learning domains. Built upon the Quick-Tune algorithm, this tool bridges the gap between research-code and practical applications, making model finetuning accessible and efficient for practitioners.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install quicktunetool\n# or\ngit clone https://github.com/automl/quicktunetool\npip install -e quicktunetool  # Use -e for editable mode\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<p>A simple example for using Quick-Tune-Tool with a pretrained optimizer for image classification:</p> <pre><code>from qtt import QuickTuner, get_pretrained_optimizer\nfrom qtt.finetune.image.classification import fn\n\n# Load task information and meta-features\ntask_info, metafeat = extract_task_info_metafeat(\"path/to/dataset\")\n\n# Initialize the optimizer\noptimizer = get_pretrained_optimizer(\"mtlbm/micro\")\noptimizer.setup(128, metafeat)\n\n# Create QuickTuner instance and run\nqt = QuickTuner(optimizer, fn)\nqt.run(task_info, time_budget=3600)\n</code></pre> <p>This code snippet demonstrates how to run QTT on an image dataset in just a few lines of code.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are welcome! Please follow these steps:</p> <ol> <li>Fork the repository</li> <li>Create a new branch (<code>git checkout -b feature/YourFeature</code>)</li> <li>Commit your changes (<code>git commit -m 'Add your feature'</code>)</li> <li>Push to the branch (<code>git push origin feature/YourFeature</code>)</li> <li>Open a pull request</li> </ol> <p>For any questions or suggestions, please contact the maintainers.</p>"},{"location":"#project-status","title":"Project Status","text":"<ul> <li>\u2705 Active development</li> </ul>"},{"location":"#support","title":"Support","text":"<ul> <li>\ud83d\udcdd Documentation</li> <li>\ud83d\udc1b Issue Tracker</li> <li>\ud83d\udcac Discussions</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the BSD License - see the LICENSE file for details.</p>"},{"location":"#references","title":"References","text":"<p>The concepts and methodologies of QuickTuneTool are detailed in the following workshop paper:</p> <pre><code>@inproceedings{\nrapant2024quicktunetool,\ntitle={Quick-Tune-Tool: A Practical Tool and its User Guide for Automatically Finetuning Pretrained Models},\nauthor={Ivo Rapant and Lennart Purucker and Fabio Ferreira and Sebastian Pineda Arango and Arlind Kadra and Josif Grabocka and Frank Hutter},\nbooktitle={AutoML Conference 2024 (Workshop Track)},\nyear={2024},\nurl={https://openreview.net/forum?id=d0Hapti3Uc}\n}\n</code></pre> <p>If you use QuickTuneTool in your research, please also cite the following paper:</p> <pre><code>@inproceedings{\narango2024quicktune,\ntitle={Quick-Tune: Quickly Learning Which Pretrained Model to Finetune and How},\nauthor={Sebastian Pineda Arango and Fabio Ferreira and Arlind Kadra and Frank Hutter and Josif Grabocka},\nbooktitle={The Twelfth International Conference on Learning Representations},\nyear={2024},\nurl={https://openreview.net/forum?id=tqh1zdXIra}\n}\n</code></pre> <p>Made with \u2764\ufe0f by @automl</p>"},{"location":"examples/","title":"Examples","text":"<p>This houses the examples for the project. Use the navigation bar to the left to view more.</p>"},{"location":"examples/define_search_space/","title":"Define Search Space","text":"Expand to copy <code>examples/define_search_space.py</code>  (top right) <pre><code>from ConfigSpace import (\n    Categorical,\n    ConfigurationSpace,\n    EqualsCondition,\n    OrConjunction,\n    OrdinalHyperparameter,\n)\n\ncs = ConfigurationSpace(\"image-classification\")\n\nfreeze = OrdinalHyperparameter(\"pct-to-freeze\", [0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\nld = OrdinalHyperparameter(\"layer-decay\", [0.0, 0.65, 0.75])\nlp = OrdinalHyperparameter(\"linear-probing\", [False, True])\nsn = OrdinalHyperparameter(\"stoch-norm\", [False, True])\nsr = OrdinalHyperparameter(\"sp-reg\", [0.0, 0.0001, 0.001, 0.01, 0.1])\nd_reg = OrdinalHyperparameter(\"delta-reg\", [0.0, 0.0001, 0.001, 0.01, 0.1])\nbss = OrdinalHyperparameter(\"bss-reg\", [0.0, 0.0001, 0.001, 0.01, 0.1])\ncot = OrdinalHyperparameter(\"cotuning-reg\", [0.0, 0.5, 1.0, 2.0, 4.0])\n\nmix = OrdinalHyperparameter(\"mixup\", [0.0, 0.2, 0.4, 1.0, 2.0, 4.0, 8.0])\nmix_p = OrdinalHyperparameter(\"mixup-prob\", [0.0, 0.25, 0.5, 0.75, 1.0])\ncut = OrdinalHyperparameter(\"cutmix\", [0.0, 0.1, 0.25, 0.5, 1.0, 2.0, 4.0])\ndrop = OrdinalHyperparameter(\"drop\", [0.0, 0.1, 0.2, 0.3, 0.4])\nsmooth = OrdinalHyperparameter(\"smoothing\", [0.0, 0.05, 0.1])\nclip = OrdinalHyperparameter(\"clip-grad\", [0, 1, 10])\n\namp = OrdinalHyperparameter(\"amp\", [False, True])\nopt = Categorical(\"opt\", [\"sgd\", \"momentum\", \"adam\", \"adamw\", \"adamp\"])\nbetas = Categorical(\"opt-betas\", [\"0.9 0.999\", \"0.0 0.99\", \"0.9 0.99\", \"0.0 0.999\"])\nlr = OrdinalHyperparameter(\"lr\", [1e-05, 5e-05, 0.0001, 0.0005, 0.001, 0.005, 0.01])\nw_ep = OrdinalHyperparameter(\"warmup_epochs\", [0, 5, 10])\nw_lr = OrdinalHyperparameter(\"warmup-lr\", [0.0, 1e-05, 1e-06])\nwd = OrdinalHyperparameter(\"weight-decay\", [0, 1e-05, 0.0001, 0.001, 0.01, 0.1])\nbs = OrdinalHyperparameter(\"batch-size\", [2, 4, 8, 16, 32, 64, 128, 256, 512])\nmom = OrdinalHyperparameter(\"momentum\", [0.0, 0.8, 0.9, 0.95, 0.99])\nsched = Categorical(\"sched\", [\"cosine\", \"step\", \"multistep\", \"plateau\"])\npe = OrdinalHyperparameter(\"patience-epochs\", [2, 5, 10])\ndr = OrdinalHyperparameter(\"decay-rate\", [0.1, 0.5])\nde = OrdinalHyperparameter(\"decay-epochs\", [10, 20])\nda = Categorical(\n    \"data-augmentation\",\n    [\"auto-augment\", \"random-augment\", \"trivial-augment\", \"none\"],\n)\naa = Categorical(\"auto-augment\", [\"v0\", \"original\"])\nra_nops = OrdinalHyperparameter(\"ra-num-ops\", [2, 3])\nra_mag = OrdinalHyperparameter(\"ra-magnitude\", [9, 17])\ncond_1 = EqualsCondition(pe, sched, \"plateau\")\ncond_2 = OrConjunction(\n    EqualsCondition(dr, sched, \"step\"),\n    EqualsCondition(dr, sched, \"multistep\"),\n)\ncond_3 = OrConjunction(\n    EqualsCondition(de, sched, \"step\"),\n    EqualsCondition(de, sched, \"multistep\"),\n)\ncond_4 = EqualsCondition(mom, opt, \"momentum\")\ncond_5 = OrConjunction(\n    EqualsCondition(betas, opt, \"adam\"),\n    EqualsCondition(betas, opt, \"adamw\"),\n    EqualsCondition(betas, opt, \"adamp\"),\n)\ncond_6 = EqualsCondition(ra_nops, da, \"random-augment\")\ncond_7 = EqualsCondition(ra_mag, da, \"random-augment\")\ncond_8 = EqualsCondition(aa, da, \"auto-augment\")\ncs.add(\n    mix,\n    mix_p,\n    cut,\n    drop,\n    smooth,\n    clip,\n    freeze,\n    ld,\n    lp,\n    sn,\n    sr,\n    d_reg,\n    bss,\n    cot,\n    amp,\n    opt,\n    betas,\n    lr,\n    w_ep,\n    w_lr,\n    wd,\n    bs,\n    mom,\n    sched,\n    pe,\n    dr,\n    de,\n    da,\n    aa,\n    ra_nops,\n    ra_mag,\n    cond_1,\n    cond_2,\n    cond_3,\n    cond_4,\n    cond_5,\n    cond_6,\n    cond_7,\n    cond_8,\n)\n\nmodel = Categorical(\n    \"model\",\n    [\n        \"beit_base_patch16_384\",\n        \"beit_large_patch16_512\",\n        \"convnext_small_384_in22ft1k\",\n        \"deit3_small_patch16_384_in21ft1k\",\n        \"dla46x_c\",\n        \"edgenext_small\",\n        \"edgenext_x_small\",\n        \"edgenext_xx_small\",\n        \"mobilevit_xs\",\n        \"mobilevit_xxs\",\n        \"mobilevitv2_075\",\n        \"swinv2_base_window12to24_192to384_22kft1k\",\n        \"tf_efficientnet_b4_ns\",\n        \"tf_efficientnet_b6_ns\",\n        \"tf_efficientnet_b7_ns\",\n        \"volo_d1_384\",\n        \"volo_d3_448\",\n        \"volo_d4_448\",\n        \"volo_d5_448\",\n        \"volo_d5_512\",\n        \"xcit_nano_12_p8_384_dist\",\n        \"xcit_small_12_p8_384_dist\",\n        \"xcit_tiny_12_p8_384_dist\",\n        \"xcit_tiny_24_p8_384_dist\",\n    ],\n)\ncs.add(model)\n\ncs.to_yaml(\"space.yaml\")\n</code></pre>"},{"location":"examples/define_search_space/#description","title":"Description","text":"<p>This examples shows how to define a search space. We use ConfigSpace.</p> <p>This search space is defined for a image classification task and includes various hyperparameters that can be optimized.</p> <p>First import the necessary modules:</p> <pre><code>from ConfigSpace import (\n    Categorical,\n    ConfigurationSpace,\n    EqualsCondition,\n    OrConjunction,\n    OrdinalHyperparameter,\n)\n\ncs = ConfigurationSpace(\"image-classification\")\n</code></pre>"},{"location":"examples/define_search_space/#finetuning-parameters","title":"Finetuning Parameters","text":"<p>The finetuning parameters in this configuration space are designed to control how a pre-trained model is fine-tuned on a new dataset. Here's a breakdown of each finetuning parameter:</p> <ol> <li> <p><code>pct-to-freeze</code> (Percentage of Model to Freeze): This parameter controls the fraction of the model's layers that will be frozen during training. Freezing a layer means that its weights will not be updated. Where <code>0.0</code> means no layers are frozen, and <code>1.0</code> means all layers are frozen, except for the final classification layer.</p> </li> <li> <p><code>layer-decay</code> (Layer-wise Learning Rate Decay): Layer-wise decay is a technique where deeper layers of the model use lower learning rates than layers closer to the output.</p> </li> <li> <p><code>linear-probing</code>: When linear probing is enabled, it means the training is focused on updating only the final classification layer (linear layer), while keeping the rest of the model frozen.</p> </li> <li> <p><code>stoch-norm</code> (Stochastic Normalization): Enabling stochastic normalization during training.</p> </li> <li> <p><code>sp-reg</code> (Starting Point Regularization): This parameter controls the amount of regularization applied to the weights of the model towards the pretrained model.</p> </li> <li> <p><code>delta-reg</code> (DELTA Regularization): DELTA regularization aims to preserve the outer layer outputs of the target network.</p> </li> <li> <p><code>bss-reg</code> (Batch Spectral Shrinkage Regularization): Batch Spectral Shrinkage (BSS) regularization penalizes the spectral norm of the model's weight matrices.</p> </li> <li> <p><code>cotuning-reg</code> (Co-tuning Regularization): This parameter controls the strength of co-tuning, a method that aligns the representation of new data with the pre-trained model's representations <pre><code>freeze = OrdinalHyperparameter(\"pct-to-freeze\", [0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\nld = OrdinalHyperparameter(\"layer-decay\", [0.0, 0.65, 0.75])\nlp = OrdinalHyperparameter(\"linear-probing\", [False, True])\nsn = OrdinalHyperparameter(\"stoch-norm\", [False, True])\nsr = OrdinalHyperparameter(\"sp-reg\", [0.0, 0.0001, 0.001, 0.01, 0.1])\nd_reg = OrdinalHyperparameter(\"delta-reg\", [0.0, 0.0001, 0.001, 0.01, 0.1])\nbss = OrdinalHyperparameter(\"bss-reg\", [0.0, 0.0001, 0.001, 0.01, 0.1])\ncot = OrdinalHyperparameter(\"cotuning-reg\", [0.0, 0.5, 1.0, 2.0, 4.0])\n</code></pre></p> </li> </ol>"},{"location":"examples/define_search_space/#regularization-parameters","title":"Regularization Parameters","text":"<ul> <li> <p><code>mixup</code>: A data augmentation technique that mixes two training samples and their labels. The value determines the strength of mixing between samples.</p> </li> <li> <p><code>mixup_prob</code>: Specifies the probability of applying mixup augmentation to a given batch. A value of 0 means mixup is never applied, while 1 means it is applied to every batch.</p> </li> <li> <p><code>cutmix</code>: Another data augmentation method that combines portions of two images and their labels.</p> </li> <li> <p><code>drop</code> (Dropout): Dropout is a regularization technique where random neurons in a layer are \"dropped out\" (set to zero) during training.</p> </li> <li> <p><code>smoothing</code> (Label Smoothing): A technique that smooths the true labels, assigning a small probability to incorrect classes.</p> </li> <li> <p><code>clip_grad</code>: This controls the gradient clipping, which constrains the magnitude of gradients during backpropagation.</p> </li> </ul> <pre><code>mix = OrdinalHyperparameter(\"mixup\", [0.0, 0.2, 0.4, 1.0, 2.0, 4.0, 8.0])\nmix_p = OrdinalHyperparameter(\"mixup-prob\", [0.0, 0.25, 0.5, 0.75, 1.0])\ncut = OrdinalHyperparameter(\"cutmix\", [0.0, 0.1, 0.25, 0.5, 1.0, 2.0, 4.0])\ndrop = OrdinalHyperparameter(\"drop\", [0.0, 0.1, 0.2, 0.3, 0.4])\nsmooth = OrdinalHyperparameter(\"smoothing\", [0.0, 0.05, 0.1])\nclip = OrdinalHyperparameter(\"clip-grad\", [0, 1, 10])\n</code></pre>"},{"location":"examples/define_search_space/#optimization-parameters","title":"Optimization Parameters","text":"<pre><code>amp = OrdinalHyperparameter(\"amp\", [False, True])\nopt = Categorical(\"opt\", [\"sgd\", \"momentum\", \"adam\", \"adamw\", \"adamp\"])\nbetas = Categorical(\"opt-betas\", [\"0.9 0.999\", \"0.0 0.99\", \"0.9 0.99\", \"0.0 0.999\"])\nlr = OrdinalHyperparameter(\"lr\", [1e-05, 5e-05, 0.0001, 0.0005, 0.001, 0.005, 0.01])\nw_ep = OrdinalHyperparameter(\"warmup_epochs\", [0, 5, 10])\nw_lr = OrdinalHyperparameter(\"warmup-lr\", [0.0, 1e-05, 1e-06])\nwd = OrdinalHyperparameter(\"weight-decay\", [0, 1e-05, 0.0001, 0.001, 0.01, 0.1])\nbs = OrdinalHyperparameter(\"batch-size\", [2, 4, 8, 16, 32, 64, 128, 256, 512])\nmom = OrdinalHyperparameter(\"momentum\", [0.0, 0.8, 0.9, 0.95, 0.99])\nsched = Categorical(\"sched\", [\"cosine\", \"step\", \"multistep\", \"plateau\"])\npe = OrdinalHyperparameter(\"patience-epochs\", [2, 5, 10])\ndr = OrdinalHyperparameter(\"decay-rate\", [0.1, 0.5])\nde = OrdinalHyperparameter(\"decay-epochs\", [10, 20])\nda = Categorical(\n    \"data-augmentation\",\n    [\"auto-augment\", \"random-augment\", \"trivial-augment\", \"none\"],\n)\naa = Categorical(\"auto-augment\", [\"v0\", \"original\"])\nra_nops = OrdinalHyperparameter(\"ra-num-ops\", [2, 3])\nra_mag = OrdinalHyperparameter(\"ra-magnitude\", [9, 17])\ncond_1 = EqualsCondition(pe, sched, \"plateau\")\ncond_2 = OrConjunction(\n    EqualsCondition(dr, sched, \"step\"),\n    EqualsCondition(dr, sched, \"multistep\"),\n)\ncond_3 = OrConjunction(\n    EqualsCondition(de, sched, \"step\"),\n    EqualsCondition(de, sched, \"multistep\"),\n)\ncond_4 = EqualsCondition(mom, opt, \"momentum\")\ncond_5 = OrConjunction(\n    EqualsCondition(betas, opt, \"adam\"),\n    EqualsCondition(betas, opt, \"adamw\"),\n    EqualsCondition(betas, opt, \"adamp\"),\n)\ncond_6 = EqualsCondition(ra_nops, da, \"random-augment\")\ncond_7 = EqualsCondition(ra_mag, da, \"random-augment\")\ncond_8 = EqualsCondition(aa, da, \"auto-augment\")\ncs.add(\n    mix,\n    mix_p,\n    cut,\n    drop,\n    smooth,\n    clip,\n    freeze,\n    ld,\n    lp,\n    sn,\n    sr,\n    d_reg,\n    bss,\n    cot,\n    amp,\n    opt,\n    betas,\n    lr,\n    w_ep,\n    w_lr,\n    wd,\n    bs,\n    mom,\n    sched,\n    pe,\n    dr,\n    de,\n    da,\n    aa,\n    ra_nops,\n    ra_mag,\n    cond_1,\n    cond_2,\n    cond_3,\n    cond_4,\n    cond_5,\n    cond_6,\n    cond_7,\n    cond_8,\n)\n</code></pre>"},{"location":"examples/define_search_space/#model-choices","title":"Model Choices","text":"<p>The model choices represent a range of state-of-the-art deep learning architectures for image classification tasks. Each model has different characteristics in terms of architecture, size, and computational efficiency, providing flexibility to users depending on their specific needs and resources. Here's an overview:</p> <ul> <li> <p>Transformer-based models: These models, such as BEiT and DeiT, use the transformer architecture that has become popular in computer vision tasks. They are highly scalable and effective for large datasets and benefit from pre-training on extensive image corpora.</p> </li> <li> <p>ConvNet-based models: Models like ConvNeXt and EfficientNet are based on convolutional neural networks (CNNs), which have long been the standard for image classification.</p> </li> <li> <p>Lightweight models: Options such as MobileViT and EdgeNeXt are designed for resource-constrained environments like mobile devices or edge computing. These models prioritize smaller size and lower computational costs. <pre><code>model = Categorical(\n    \"model\",\n    [\n        \"beit_base_patch16_384\",\n        \"beit_large_patch16_512\",\n        \"convnext_small_384_in22ft1k\",\n        \"deit3_small_patch16_384_in21ft1k\",\n        \"dla46x_c\",\n        \"edgenext_small\",\n        \"edgenext_x_small\",\n        \"edgenext_xx_small\",\n        \"mobilevit_xs\",\n        \"mobilevit_xxs\",\n        \"mobilevitv2_075\",\n        \"swinv2_base_window12to24_192to384_22kft1k\",\n        \"tf_efficientnet_b4_ns\",\n        \"tf_efficientnet_b6_ns\",\n        \"tf_efficientnet_b7_ns\",\n        \"volo_d1_384\",\n        \"volo_d3_448\",\n        \"volo_d4_448\",\n        \"volo_d5_448\",\n        \"volo_d5_512\",\n        \"xcit_nano_12_p8_384_dist\",\n        \"xcit_small_12_p8_384_dist\",\n        \"xcit_tiny_12_p8_384_dist\",\n        \"xcit_tiny_24_p8_384_dist\",\n    ],\n)\ncs.add(model)\n\ncs.to_yaml(\"space.yaml\")\n</code></pre></p> </li> </ul>"},{"location":"examples/metatrain/","title":"Metatrain","text":"Expand to copy <code>examples/metatrain.py</code>  (top right) <pre><code>The `fit`-method of the predictors takes tabular data as input. If the data is stored in\na CSV file, the expected format of the CSV is shown below:\n\n## Configurations\n\nHyperparammeter configurations of previous evaluations. Do not apply any preprocessing\nto the data. Use native data types as much as possible.\n\n|           | model         | opt   | lr     | sched   | batch_size |\n|-----------|---------------|-------|--------|---------|------------|\n| 1         | xcit_abc      | adam  | 0.001  | cosine  | 64         |\n| 2         | beit_def      | sgd   | 0.0005 | step    | 128        |\n| 3         | mobilevit_xyz | adamw | 0.01   | plateau | 32         |\n| ... |\n\n## Meta-Features\n\nMeta-features are optional. Meta-features refer to features that describe or summarize\nother features in a dataset. They are higher-level characteristics or properties of the\ndataset that can provide insight into its structure or complexity.\n\n|   | num-features | num-classes |\n|---|--------------|-------------|\n| 1 | 128          | 42          |\n| 2 | 256          | 123         |\n| 3 | 384          | 1000        |\n\n## Learning Curves\n\nLearning curves show the performance of a model over time or over iterations as it\nlearns from training data. For the vision classification task, the learning curves\nare the validation accuracy on the validation set.\n\n|   |  1   |  2   |  3   |  4   |  5   | ... |\n|---|------|------|------|------|------|-----|\n| 1 | 0.11 | 0.12 | 0.13 | 0.14 | 0.15 | ... |\n| 2 | 0.21 | 0.22 | 0.23 | 0.24 | 0.25 | ... |\n| 3 | 0.31 | 0.32 | 0.33 | 0.34 | 0.35 | ... |\n\n## Cost\n\nThe cost of running a pipeline (per fidelity). This refers to the total runtime required\nto complete the pipeline. This includes both the training and evaluation phases. We use\nthe total runtime as the cost measure for each pipeline execution.\n\n|   | cost  |\n|---|-------|\n| 1 | 12.3  |\n| 2 | 45.6  |\n| 3 | 78.9  |\n\nEnsure that the CSV files follow this structure for proper processing.\n</code></pre>"},{"location":"examples/metatrain/#description","title":"Description","text":"<p>from qtt.predictors import PerfPredictor, CostPredictor import pandas as pd</p> <p><pre><code>The `fit`-method of the predictors takes tabular data as input. If the data is stored in\na CSV file, the expected format of the CSV is shown below:\n\n## Configurations\n\nHyperparammeter configurations of previous evaluations. Do not apply any preprocessing\nto the data. Use native data types as much as possible.\n\n|           | model         | opt   | lr     | sched   | batch_size |\n|-----------|---------------|-------|--------|---------|------------|\n| 1         | xcit_abc      | adam  | 0.001  | cosine  | 64         |\n| 2         | beit_def      | sgd   | 0.0005 | step    | 128        |\n| 3         | mobilevit_xyz | adamw | 0.01   | plateau | 32         |\n| ... |\n\n## Meta-Features\n\nMeta-features are optional. Meta-features refer to features that describe or summarize\nother features in a dataset. They are higher-level characteristics or properties of the\ndataset that can provide insight into its structure or complexity.\n\n|   | num-features | num-classes |\n|---|--------------|-------------|\n| 1 | 128          | 42          |\n| 2 | 256          | 123         |\n| 3 | 384          | 1000        |\n\n## Learning Curves\n\nLearning curves show the performance of a model over time or over iterations as it\nlearns from training data. For the vision classification task, the learning curves\nare the validation accuracy on the validation set.\n\n|   |  1   |  2   |  3   |  4   |  5   | ... |\n|---|------|------|------|------|------|-----|\n| 1 | 0.11 | 0.12 | 0.13 | 0.14 | 0.15 | ... |\n| 2 | 0.21 | 0.22 | 0.23 | 0.24 | 0.25 | ... |\n| 3 | 0.31 | 0.32 | 0.33 | 0.34 | 0.35 | ... |\n\n## Cost\n\nThe cost of running a pipeline (per fidelity). This refers to the total runtime required\nto complete the pipeline. This includes both the training and evaluation phases. We use\nthe total runtime as the cost measure for each pipeline execution.\n\n|   | cost  |\n|---|-------|\n| 1 | 12.3  |\n| 2 | 45.6  |\n| 3 | 78.9  |\n\nEnsure that the CSV files follow this structure for proper processing.\n</code></pre> config = pd.read_csv(\"config.csv\", index_col=0)  # pipeline configurations meta = pd.read_csv(\"meta.csv\", index_col=0)  # if meta-features are available curve = pd.read_csv(\"curve.csv\", index_col=0)  # learning curves cost = pd.read_csv(\"cost.csv\", index_col=0)  # runtime costs</p> <p>X = pd.concat([config, meta], axis=1) curve = curve.values  # predictors expect curves as numpy arrays cost = cost.values  # predictors expect costs as numpy arrays</p> <p>perf_predictor = PerfPredictor().fit(X, curve) cost_predictor = CostPredictor().fit(X, cost)</p>"},{"location":"examples/quicktuning/","title":"A quick example of using a special QuickTuner to tune image classifiers on a new dataset.","text":"Expand to copy <code>examples/quicktuning.py</code>  (top right) <pre><code>\n</code></pre>"},{"location":"examples/quicktuning/#description","title":"Description","text":"<p>from qtt import QuickImageCLSTuner</p> <p>tuner = QuickImageCLSTuner(\"path/to/dataset\")</p>"},{"location":"examples/step_by_step/","title":"from qtt import QuickOptimizer, QuickTuner","text":"Expand to copy <code>examples/step_by_step.py</code>  (top right) <pre><code>\n</code></pre>"},{"location":"examples/step_by_step/#description","title":"Description","text":"<p>from qtt.finetune.image.classification import extract_image_dataset_metafeat, fn import pandas as pd from ConfigSpace import ConfigurationSpace</p> <p>pipeline = pd.read_csv(\"pipeline.csv\", index_col=0) curve = pd.read_csv(\"curve.csv\", index_col=0) cost = pd.read_csv(\"cost.csv\", index_col=0) meta = pd.read_csv(\"meta.csv\", index_col=0) cs = ConfigurationSpace.from_yaml(\"space.yaml\")</p> <p>config = pd.merge(pipeline, meta, on=\"dataset\") config.drop((\"dataset\"), axis=1, inplace=True) opt = QuickOptimizer(cs, 50, cost_aware=True)</p> <p>ti, mf = extract_image_dataset_metafeat(\"path/to/dataset\") opt.setup(128, mf)</p> <p>qt = QuickTuner(opt, fn) qt.run(100, trial_info=ti)</p>"},{"location":"reference/","title":"Code References","text":"<p>This section provides references for the core components of Quick-Tune-Tool, describing the primary modules and classes that make up the tool's architecture. The code is organized into three main parts: Optimizers, Predictors, and Tuners.</p>"},{"location":"reference/#1-optimizers","title":"1. Optimizers","text":"<p>The Optimizers module is responsible for suggesting configurations for evaluation, using various optimization strategies. Available optimizers include:</p> <ul> <li>QuickTune Optimizer </li> <li>File: <code>optimizers/quick.py</code></li> <li> <p>Implements the QuickTune algorithm, balancing multi-fidelity expected improvement with cost estimation to select configurations.</p> </li> <li> <p>Random Search Optimizer </p> </li> <li>File: <code>optimizers/random.py</code></li> <li>Provides a basic random search optimizer as a baseline for comparison with other optimization strategies.</li> </ul>"},{"location":"reference/#2-predictors","title":"2. Predictors","text":"<p>The Predictors module includes components that estimate model performance and finetuning costs, enabling efficient configuration selection.</p> <ul> <li> <p>Performance Predictor</p> <ul> <li>File: <code>predictors/perf.py</code></li> <li>Uses meta-learning to estimate the potential performance of a model configuration based on historical data and auxiliary task information.</li> </ul> </li> <li> <p>Cost Predictor</p> <ul> <li>File: <code>predictors/cost.py</code></li> <li>Evaluates the computational cost associated with different finetuning configurations, helping to balance resource efficiency with optimization goals.</li> </ul> </li> </ul>"},{"location":"reference/#3-tuners","title":"3. Tuners","text":"<p>The Tuners module coordinates the tuning process, managing environment setup, experiment flow, and result handling.</p> <ul> <li> <p>QuickTuner </p> <ul> <li>File: <code>tuners/quick.py</code></li> <li>Serves as the central class that manages the tuning process, integrating optimizers and predictors to manage iterative evaluations and updates.</li> </ul> </li> <li> <p>Image-Classification </p> <ul> <li>File: <code>tuners/image/classification/tuner.py</code></li> <li>A specialized tuner for image classification, offering a reduced interface where users simply provide the path to the image dataset.</li> </ul> </li> </ul>"},{"location":"reference/#additional-resources","title":"Additional Resources","text":"<ul> <li> <p>Finetuning Scripts </p> <ul> <li>Directory: <code>scripts/</code></li> <li>Functions used to evaluate configurations, returning performance metrics for each step.</li> </ul> </li> <li> <p>Utility Scripts </p> <ul> <li>Directory: <code>utils/</code></li> <li>A collection of helper functions and utilities to support data processing, result logging, and other ancillary tasks.</li> </ul> </li> </ul> <p>Refer to each module's in-code documentation for further details on function arguments, usage examples, and dependencies.</p>"},{"location":"reference/optimizers/","title":"Overview","text":"<p>The <code>Optimizer</code> class serves as a base class within the Quick-Tune-Tool, providing low-level functionality. It is designed to support flexible configuration management and interact with tuners during the optimization process. Key aspects of the class include directory setup, model saving, and interfacing methods for requesting and reporting trial evaluations.</p> <p>Here\u2019s an overview of the <code>Optimizer</code> class:</p>"},{"location":"reference/optimizers/#core-methods","title":"Core Methods","text":"<ul> <li> <p><code>ask</code>: Abstract method that must be implemented in subclasses. It requests a new configuration trial from the optimizer, returning it as a dictionary. Raises <code>NotImplementedError</code> if not overridden.</p> </li> <li> <p><code>tell</code>: Accepts and processes a report (result) from a trial evaluation. This method allows the optimizer to record outcomes for each configuration and adjust future suggestions accordingly. Supports both single and multiple trial reports.</p> </li> <li> <p><code>ante</code>: A placeholder method for pre-processing tasks to be performed before requesting a configuration trial (used by tuners). Can be overridden in subclasses for custom pre-processing.</p> </li> <li> <p><code>post</code>: A placeholder for post-processing tasks, executed after a trial evaluation has been submitted. Designed</p> </li> </ul> <p>This class is intended to be extended for specific optimization strategies, with <code>ask</code> and <code>tell</code> as the primary methods for interaction with tuners.</p>"},{"location":"reference/optimizers/#available-optimizers","title":"Available Optimizers","text":"<ul> <li><code>RandomOptimizer</code></li> <li><code>QuickOptimizer</code></li> </ul>"},{"location":"reference/optimizers/optimizer/","title":"Optimizer","text":""},{"location":"reference/optimizers/optimizer/#qtt.optimizers.optimizer.Optimizer","title":"<code>Optimizer</code>","text":"<p>Base class. Implements all low-level functionality.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Name of the subdirectory inside path where model will be saved. The final model directory will be os.path.join(path, name) If None, defaults to the model's class name: self.class.name</p> </li> <li> <code>path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Directory location to store all outputs. If None, a new unique time-stamped directory is chosen.</p> </li> </ul> Source code in <code>src/qtt/optimizers/optimizer.py</code> <pre><code>class Optimizer:\n    \"\"\"Base class. Implements all low-level functionality.\n\n    Args:\n        name (str):\n            Name of the subdirectory inside path where model will be saved.\n            The final model directory will be os.path.join(path, name)\n            If None, defaults to the model's class name: self.__class__.__name__\n        path (str):\n            Directory location to store all outputs.\n            If None, a new unique time-stamped directory is chosen.\n    \"\"\"\n\n    model_file_name = \"model.pkl\"\n\n    def __init__(\n        self,\n        name: str | None = None,\n        path: str | None = None,\n    ):\n        if name is None:\n            self.name = self.__class__.__name__\n            logger.info(\n                f\"No name was specified for model, defaulting to class name: {self.name}\",\n            )\n        else:\n            self.name = name\n\n        if path is None:\n            self.path = setup_outputdir(path=self.name.lower())\n            logger.info(\n                f\"No path was specified for predictor, defaulting to: {self.path}\",\n            )\n        else:\n            self.path = setup_outputdir(path)\n\n        self._is_initialized = False\n\n    def ante(self):\n        \"\"\"This method is intended for the use with a tuner.\n        It allows to perform some pre-processing steps before each ask.\"\"\"\n        pass\n\n    def post(self):\n        \"\"\"This method is intended for the use with a tuner.\n        It allows to perform some post-processing steps after each tell.\"\"\"\n        pass\n\n    def ask(self) -&gt; dict | None:\n        \"\"\"Ask the optimizer for a trial to evaluate.\n\n        Returns:\n            A config to sample.\n        \"\"\"\n        raise NotImplementedError\n\n    def tell(self, report: dict | list[dict]):\n        \"\"\"Tell the optimizer the result for an asked trial.\n\n        Args:\n            report (dict): The result for a trial\n        \"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    def load(cls, path: str, reset_paths: bool = True, verbose: bool = True):\n        \"\"\"\n        Loads the model from disk to memory.\n\n        Args:\n            path (str):\n                Path to the saved model, minus the file name.\n                This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS).\n                The model file is typically located in os.path.join(path, cls.model_file_name).\n            reset_paths (bool):\n                Whether to reset the self.path value of the loaded model to be equal to path.\n                It is highly recommended to keep this value as True unless accessing the original self.path value is important.\n                If False, the actual valid path and self.path may differ, leading to strange behaviour and potential exceptions if the model needs to load any other files at a later time.\n            verbose (bool):\n                Whether to log the location of the loaded file.\n\n        Returns:\n            model (Optimizer): The loaded model object.\n        \"\"\"\n        file_path = os.path.join(path, cls.model_file_name)\n        with open(file_path, \"rb\") as f:\n            model = pickle.load(f)\n        if reset_paths:\n            model.path = path\n        if verbose:\n            logger.info(f\"Model loaded from: {file_path}\")\n        return model\n\n    def save(self, path: str | None = None, verbose: bool = True) -&gt; str:\n        \"\"\"\n        Saves the model to disk.\n\n        Args:\n            path (str): Path to the saved model, minus the file name. This should generally\n                be a directory path ending with a '/' character (or appropriate path separator\n                value depending on OS). If None, self.path is used. The final model file is\n                typically saved to os.path.join(path, self.model_file_name).\n            verbose (bool): Whether to log the location of the saved file.\n\n        Returns:\n            str: Path to the saved model, minus the file name. Use this value to load the\n                model from disk via cls.load(path), where cls is the class of the model\n                object (e.g., model = Model.load(path)).\n        \"\"\"\n        if path is None:\n            path = self.path\n        os.makedirs(path, exist_ok=True)\n        file_path = os.path.join(path, self.model_file_name)\n        with open(file_path, \"wb\") as f:\n            pickle.dump(self, f, protocol=pickle.HIGHEST_PROTOCOL)\n        if verbose:\n            logger.info(f\"Model saved to: {file_path}\")\n        return path\n\n    def reset_path(self, path: str | None = None):\n        \"\"\"Reset the path of the model.\n\n        Args:\n            path (str):\n                Directory location to store all outputs.\n                If None, a new unique time-stamped directory is chosen.\n        \"\"\"\n        if path is None:\n            path = setup_outputdir(path=self.name.lower(), path_suffix=self.name)\n        self.path = path\n</code></pre>"},{"location":"reference/optimizers/optimizer/#qtt.optimizers.optimizer.Optimizer.ante","title":"<code>ante()</code>","text":"<p>This method is intended for the use with a tuner. It allows to perform some pre-processing steps before each ask.</p> Source code in <code>src/qtt/optimizers/optimizer.py</code> <pre><code>def ante(self):\n    \"\"\"This method is intended for the use with a tuner.\n    It allows to perform some pre-processing steps before each ask.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/optimizers/optimizer/#qtt.optimizers.optimizer.Optimizer.ask","title":"<code>ask()</code>","text":"<p>Ask the optimizer for a trial to evaluate.</p> <p>Returns:</p> <ul> <li> <code>dict | None</code>           \u2013            <p>A config to sample.</p> </li> </ul> Source code in <code>src/qtt/optimizers/optimizer.py</code> <pre><code>def ask(self) -&gt; dict | None:\n    \"\"\"Ask the optimizer for a trial to evaluate.\n\n    Returns:\n        A config to sample.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/optimizers/optimizer/#qtt.optimizers.optimizer.Optimizer.load","title":"<code>load(path, reset_paths=True, verbose=True)</code>  <code>classmethod</code>","text":"<p>Loads the model from disk to memory.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>Path to the saved model, minus the file name. This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS). The model file is typically located in os.path.join(path, cls.model_file_name).</p> </li> <li> <code>reset_paths</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to reset the self.path value of the loaded model to be equal to path. It is highly recommended to keep this value as True unless accessing the original self.path value is important. If False, the actual valid path and self.path may differ, leading to strange behaviour and potential exceptions if the model needs to load any other files at a later time.</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to log the location of the loaded file.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>model</code> (              <code>Optimizer</code> )          \u2013            <p>The loaded model object.</p> </li> </ul> Source code in <code>src/qtt/optimizers/optimizer.py</code> <pre><code>@classmethod\ndef load(cls, path: str, reset_paths: bool = True, verbose: bool = True):\n    \"\"\"\n    Loads the model from disk to memory.\n\n    Args:\n        path (str):\n            Path to the saved model, minus the file name.\n            This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS).\n            The model file is typically located in os.path.join(path, cls.model_file_name).\n        reset_paths (bool):\n            Whether to reset the self.path value of the loaded model to be equal to path.\n            It is highly recommended to keep this value as True unless accessing the original self.path value is important.\n            If False, the actual valid path and self.path may differ, leading to strange behaviour and potential exceptions if the model needs to load any other files at a later time.\n        verbose (bool):\n            Whether to log the location of the loaded file.\n\n    Returns:\n        model (Optimizer): The loaded model object.\n    \"\"\"\n    file_path = os.path.join(path, cls.model_file_name)\n    with open(file_path, \"rb\") as f:\n        model = pickle.load(f)\n    if reset_paths:\n        model.path = path\n    if verbose:\n        logger.info(f\"Model loaded from: {file_path}\")\n    return model\n</code></pre>"},{"location":"reference/optimizers/optimizer/#qtt.optimizers.optimizer.Optimizer.post","title":"<code>post()</code>","text":"<p>This method is intended for the use with a tuner. It allows to perform some post-processing steps after each tell.</p> Source code in <code>src/qtt/optimizers/optimizer.py</code> <pre><code>def post(self):\n    \"\"\"This method is intended for the use with a tuner.\n    It allows to perform some post-processing steps after each tell.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/optimizers/optimizer/#qtt.optimizers.optimizer.Optimizer.reset_path","title":"<code>reset_path(path=None)</code>","text":"<p>Reset the path of the model.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Directory location to store all outputs. If None, a new unique time-stamped directory is chosen.</p> </li> </ul> Source code in <code>src/qtt/optimizers/optimizer.py</code> <pre><code>def reset_path(self, path: str | None = None):\n    \"\"\"Reset the path of the model.\n\n    Args:\n        path (str):\n            Directory location to store all outputs.\n            If None, a new unique time-stamped directory is chosen.\n    \"\"\"\n    if path is None:\n        path = setup_outputdir(path=self.name.lower(), path_suffix=self.name)\n    self.path = path\n</code></pre>"},{"location":"reference/optimizers/optimizer/#qtt.optimizers.optimizer.Optimizer.save","title":"<code>save(path=None, verbose=True)</code>","text":"<p>Saves the model to disk.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Path to the saved model, minus the file name. This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS). If None, self.path is used. The final model file is typically saved to os.path.join(path, self.model_file_name).</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to log the location of the saved file.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>Path to the saved model, minus the file name. Use this value to load the     model from disk via cls.load(path), where cls is the class of the model     object (e.g., model = Model.load(path)).</p> </li> </ul> Source code in <code>src/qtt/optimizers/optimizer.py</code> <pre><code>def save(self, path: str | None = None, verbose: bool = True) -&gt; str:\n    \"\"\"\n    Saves the model to disk.\n\n    Args:\n        path (str): Path to the saved model, minus the file name. This should generally\n            be a directory path ending with a '/' character (or appropriate path separator\n            value depending on OS). If None, self.path is used. The final model file is\n            typically saved to os.path.join(path, self.model_file_name).\n        verbose (bool): Whether to log the location of the saved file.\n\n    Returns:\n        str: Path to the saved model, minus the file name. Use this value to load the\n            model from disk via cls.load(path), where cls is the class of the model\n            object (e.g., model = Model.load(path)).\n    \"\"\"\n    if path is None:\n        path = self.path\n    os.makedirs(path, exist_ok=True)\n    file_path = os.path.join(path, self.model_file_name)\n    with open(file_path, \"wb\") as f:\n        pickle.dump(self, f, protocol=pickle.HIGHEST_PROTOCOL)\n    if verbose:\n        logger.info(f\"Model saved to: {file_path}\")\n    return path\n</code></pre>"},{"location":"reference/optimizers/optimizer/#qtt.optimizers.optimizer.Optimizer.tell","title":"<code>tell(report)</code>","text":"<p>Tell the optimizer the result for an asked trial.</p> <p>Parameters:</p> <ul> <li> <code>report</code>               (<code>dict</code>)           \u2013            <p>The result for a trial</p> </li> </ul> Source code in <code>src/qtt/optimizers/optimizer.py</code> <pre><code>def tell(self, report: dict | list[dict]):\n    \"\"\"Tell the optimizer the result for an asked trial.\n\n    Args:\n        report (dict): The result for a trial\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/optimizers/quick/","title":"QuickOptimizer","text":""},{"location":"reference/optimizers/quick/#qtt.optimizers.quick.QuickOptimizer","title":"<code>QuickOptimizer</code>","text":"<p>               Bases: <code>Optimizer</code></p> <p>QuickOptimizer implements a cost-aware Bayesian optimization. It builds upon the DyHPO algorithm, adding cost-awareness to the optimization process.</p> <p>Parameters:</p> <ul> <li> <code>cs</code>               (<code>ConfigurationSpace</code>)           \u2013            <p>The configuration space to optimize over.</p> </li> <li> <code>max_fidelity</code>               (<code>int</code>)           \u2013            <p>The maximum fidelity to optimize. Fidelity is a measure of a resource used by a configuration, such as the number of epochs.</p> </li> <li> <code>perf_predictor</code>               (<code>PerfPredictor</code>, default:                   <code>None</code> )           \u2013            <p>The performance predictor to use. If None, a new predictor is created.</p> </li> <li> <code>cost_predictor</code>               (<code>CostPredictor</code>, default:                   <code>None</code> )           \u2013            <p>The cost predictor to use. If None, a new CostPredictor is created if <code>cost_aware</code> is True.</p> </li> <li> <code>cost_aware</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use the cost predictor. Defaults to False.</p> </li> <li> <code>cost_factor</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>A factor to control the scaling of cost values. Values must be in the range <code>[0.0, inf)</code>. A cost factor smaller than 1 compresses the cost values closer together (with 0 equalizing them), while values larger than 1 expand them. Defaults to 1.0.</p> </li> <li> <code>acq_fn</code>               (<code>str</code>, default:                   <code>'ei'</code> )           \u2013            <p>The acquisition function to use. One of [\"ei\", \"ucb\", \"thompson\", \"exploit\"]. Defaults to \"ei\".</p> </li> <li> <code>explore_factor</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>The exploration factor in the acquisition function. Defaults to 1.0.</p> </li> <li> <code>patience</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Determines if early stopping should be applied for a single configuration. If the score does not improve for <code>patience</code> steps, the configuration is stopped. Defaults to None.</p> </li> <li> <code>tol</code>               (<code>float</code>, default:                   <code>0.0001</code> )           \u2013            <p>Tolerance for early stopping. Training stops if the score does not improve by at least <code>tol</code> for <code>patience</code> iterations (if set). Values must be in the range <code>[0.0, inf)</code>. Defaults to 0.0.</p> </li> <li> <code>score_thresh</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Threshold for early stopping. If the score is above <code>1 - score_thresh</code>, the configuration is stopped. Defaults to 0.0.</p> </li> <li> <code>init_random_search_steps</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>Number of configurations to evaluate randomly at the beginning of the optimization (with fidelity 1) before using predictors/acquisition function. Defaults to 10.</p> </li> <li> <code>refit_init_steps</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of steps (successful evaluations) before refitting the predictors. Defaults to 0.</p> </li> <li> <code>refit</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to refit the predictors with observed data. Defaults to False.</p> </li> <li> <code>refit_interval</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Interval for refitting the predictors. Defaults to 1.</p> </li> <li> <code>path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Path to save the optimizer state. Defaults to None.</p> </li> <li> <code>seed</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Seed for reproducibility. Defaults to None.</p> </li> <li> <code>verbosity</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <p>Verbosity level for logging. Defaults to 2.</p> </li> </ul> Source code in <code>src/qtt/optimizers/quick.py</code> <pre><code>class QuickOptimizer(Optimizer):\n    \"\"\"QuickOptimizer implements a cost-aware Bayesian optimization. It builds upon the\n    DyHPO algorithm, adding cost-awareness to the optimization process.\n\n    Args:\n        cs (ConfigurationSpace): The configuration space to optimize over.\n        max_fidelity (int): The maximum fidelity to optimize. Fidelity is a measure of\n            a resource used by a configuration, such as the number of epochs.\n        perf_predictor (PerfPredictor, optional): The performance predictor to use. If\n            None, a new predictor is created.\n        cost_predictor (CostPredictor, optional): The cost predictor to use. If None,\n            a new CostPredictor is created if `cost_aware` is True.\n        cost_aware (bool, optional): Whether to use the cost predictor. Defaults to False.\n        cost_factor (float, optional): A factor to control the scaling of cost values.\n            Values must be in the range `[0.0, inf)`. A cost factor smaller than 1\n            compresses the cost values closer together (with 0 equalizing them), while\n            values larger than 1 expand them. Defaults to 1.0.\n        acq_fn (str, optional): The acquisition function to use. One of [\"ei\", \"ucb\",\n            \"thompson\", \"exploit\"]. Defaults to \"ei\".\n        explore_factor (float, optional): The exploration factor in the acquisition\n            function. Defaults to 1.0.\n        patience (int, optional): Determines if early stopping should be applied for a\n            single configuration. If the score does not improve for `patience` steps,\n            the configuration is stopped. Defaults to None.\n        tol (float, optional): Tolerance for early stopping. Training stops if the score\n            does not improve by at least `tol` for `patience` iterations (if set). Values\n            must be in the range `[0.0, inf)`. Defaults to 0.0.\n        score_thresh (float, optional): Threshold for early stopping. If the score is\n            above `1 - score_thresh`, the configuration is stopped. Defaults to 0.0.\n        init_random_search_steps (int, optional): Number of configurations to evaluate\n            randomly at the beginning of the optimization (with fidelity 1) before using\n            predictors/acquisition function. Defaults to 10.\n        refit_init_steps (int, optional): Number of steps (successful evaluations) before\n            refitting the predictors. Defaults to 0.\n        refit (bool, optional): Whether to refit the predictors with observed data.\n            Defaults to False.\n        refit_interval (int, optional): Interval for refitting the predictors. Defaults\n            to 1.\n        path (str, optional): Path to save the optimizer state. Defaults to None.\n        seed (int, optional): Seed for reproducibility. Defaults to None.\n        verbosity (int, optional): Verbosity level for logging. Defaults to 2.\n    \"\"\"\n\n    def __init__(\n        self,\n        cs: ConfigurationSpace,\n        max_fidelity: int,\n        perf_predictor: PerfPredictor | None = None,\n        cost_predictor: CostPredictor | None = None,\n        *,\n        cost_aware: bool = False,\n        cost_factor: float = 1.0,\n        acq_fn: Literal[\"ei\", \"ucb\", \"thompson\", \"exploit\"] = \"ei\",\n        explore_factor: float = 0.0,\n        patience: int | None = None,\n        tol: float = 1e-4,\n        score_thresh: float = 0.0,\n        init_random_search_steps: int = 3,\n        refit_init_steps: int = 0,\n        refit: bool = False,\n        refit_interval: int = 1,\n        #\n        path: str | None = None,\n        seed: int | None = None,\n        verbosity: int = 2,\n    ):\n        super().__init__(path=path)\n        set_logger_verbosity(verbosity, logger)\n        self.verbosity = verbosity\n\n        if seed is not None:\n            fix_random_seeds(seed)\n        self.seed = seed\n\n        # configuration space\n        self.cs = cs\n        self.max_fidelity = max_fidelity\n\n        # optimizer related parameters\n        self.acq_fn = acq_fn\n        self.explore_factor = explore_factor\n        self.cost_aware = cost_aware\n        self.cost_factor = cost_factor\n        self.patience = patience\n        self.tol = tol\n        self.scr_thr = score_thresh\n        self.refit_init_steps = refit_init_steps\n        self.refit = refit\n        self.refit_interval = refit_interval\n\n        # predictors\n        self.perf_predictor = perf_predictor\n        if self.perf_predictor is None:\n            self.perf_predictor = PerfPredictor(path=path)\n        self.cost_predictor = cost_predictor\n        if self.cost_aware and self.cost_predictor is None:\n            self.cost_predictor = CostPredictor(path=path)\n\n        # trackers\n        self.init_random_search_steps = init_random_search_steps\n        self.ask_count = 0\n        self.tell_count = 0\n        self.init_count = 0\n        self.eval_count = 0\n        self.configs: list[dict] = []\n        self.evaled: set[int] = set()\n        self.stoped: set[int] = set()\n        self.failed: set[int] = set()\n        self.history: list = []\n\n        # placeholders\n        self.pipelines: pd.DataFrame\n        self.curves: np.ndarray\n        self.fidelities: np.ndarray\n        self.costs: np.ndarray | None = None\n        self.score_history: np.ndarray | None = None\n\n        # flags\n        self.ready = False\n        self.finished = False\n\n    def setup(\n        self,\n        n: int,\n        metafeat: Mapping[str, int | float] | None = None,\n    ) -&gt; None:\n        \"\"\"Setup the optimizer for optimization.\n\n        Create the configurations to evaluate. The configurations are sampled from the\n        configuration space. Optionally, metafeatures of the dataset can be provided.\n\n        Args:\n            n (int): The number of configurations to create.\n            metafeat (Mapping[str, int | float], optional): The metafeatures of the dataset.\n        \"\"\"\n        self.N = n\n        self.fidelities = np.zeros(n, dtype=int)\n        self.curves = np.full((n, self.max_fidelity), np.nan, dtype=float)\n        self.costs = None\n        if self.patience is not None:\n            self.score_history = np.zeros((n, self.patience), dtype=float)\n\n        if self.seed is not None:\n            self.cs.seed(self.seed)\n        _configs = self.cs.sample_configuration(n)\n        self.configs = [dict(c) for c in _configs]\n        self.pipelines = pd.DataFrame(self.configs)\n\n        self.metafeat = None\n        if metafeat is not None:\n            self.metafeat = pd.DataFrame([metafeat] * self.N)\n        self.pipelines = pd.concat([self.pipelines, self.metafeat], axis=1)\n\n        self.ready = True\n\n    def setup_pandas(\n        self,\n        df: pd.DataFrame,\n        metafeat: Mapping[str, int | float] | None = None,\n    ):\n        \"\"\"Setup the optimizer for optimization.\n\n        Use an existing DataFrame to create the configurations to evaluate. Optionally,\n        metafeatures of the dataset can be provided.\n\n        Args:\n            df (pd.DataFrame): The DataFrame with the configurations to evaluate.\n            metafeat (Mapping[str, int | float], optional): The metafeatures of the dataset.\n        \"\"\"\n        self.pipelines = df\n        self.N = len(df)\n        self.fidelities = np.zeros(self.N, dtype=int)\n        self.curves = np.full((self.N, self.max_fidelity), np.nan, dtype=float)\n        self.costs = None\n        if self.patience is not None:\n            self.score_history = np.zeros((self.N, self.patience), dtype=float)\n\n        self.metafeat = None\n        if metafeat is not None:\n            self.metafeat = pd.DataFrame([metafeat] * self.N)\n        self.pipelines = pd.concat([self.pipelines, self.metafeat], axis=1)\n        self.configs = self.pipelines.to_dict(orient=\"records\")\n\n        self.ready = True\n\n    def _predict(self) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray | None]:\n        \"\"\"Predict the performance and cost of the configurations.\n\n        Returns:\n            The mean and standard deviation of the performance of the pipelines and their costs.\n        \"\"\"\n        pipeline, curve = self.pipelines, self.curves\n\n        if self.perf_predictor is None:\n            raise AssertionError(\"PerfPredictor is not fitted yet\")\n        pred = self.perf_predictor.predict(X=pipeline, curve=curve)\n        pred_mean, pred_std = pred\n\n        costs = None\n        if self.cost_aware:\n            if self.cost_predictor is None:\n                raise AssertionError(\"CostPredictor is not fitted yet\")\n            if self.costs is None:\n                c = self.cost_predictor.predict(X=pipeline)\n                c = np.clip(c, 1e-6, None)  # avoid division by zero\n                c /= c.max()  # normalize\n                c = np.power(c, self.cost_factor)  # rescale\n                self.costs = c\n            costs = self.costs\n\n        return pred_mean, pred_std, costs\n\n    def _calc_acq_val(self, mean, std, y_max):\n        \"\"\"Calculate the acquisition value.\n\n        Args:\n            mean (np.ndarray): The mean of the predictions.\n            std (np.ndarray): The standard deviation of the predictions.\n            cost (np.ndarray): The cost of the pipeline.\n\n        Returns:\n            The acquisition values.\n        \"\"\"\n        fn = self.acq_fn\n        xi = self.explore_factor\n        match fn:\n            # Expected Improvement\n            case \"ei\":\n                mask = std == 0\n                std = std + mask * 1.0\n                z = (mean - y_max - xi) / std\n                acq_value = (mean - y_max) * norm.cdf(z) + std * norm.pdf(z)\n                acq_value[mask] = 0.0\n            # Upper Confidence Bound\n            case \"ucb\":\n                acq_value = mean + xi * std\n            # Thompson Sampling\n            case \"thompson\":\n                acq_value = np.random.normal(mean, std)\n            # Exploitation\n            case \"exploit\":\n                acq_value = mean\n            case _:\n                raise ValueError\n        return acq_value\n\n    def _optimize_acq_fn(self, mean, std, cost) -&gt; list[int]:\n        \"\"\"\n        Optimize the acquisition function.\n\n        Args:\n            mean (np.ndarray): The mean of the predictions.\n            std (np.ndarray): The standard deviation of the predictions.\n            cost (np.ndarray): The cost of the pipeline.\n\n        Returns:\n            list[int]: A sorted list of indices of the pipeline.\n        \"\"\"\n        # maximum score per fidelity\n        curves = np.nan_to_num(self.curves)\n        y_max = curves.max(axis=0)\n        y_max = np.maximum.accumulate(y_max)\n\n        # get the ymax for the next fidelity of the pipelines\n        next_fidelitys = np.minimum(self.fidelities + 1, self.max_fidelity)\n        y_max_next = y_max[next_fidelitys - 1]\n\n        acq_values = self._calc_acq_val(mean, std, y_max_next)\n        if self.cost_aware:\n            acq_values /= cost\n\n        return np.argsort(acq_values).tolist()\n\n    def _ask(self):\n        pred_mean, pred_std, cost = self._predict()\n        ranks = self._optimize_acq_fn(pred_mean, pred_std, cost)\n        ranks = [r for r in ranks if r not in self.stoped | self.failed]\n        index = ranks[-1]\n        logger.debug(f\"predicted score: {pred_mean[index]:.4f}\")\n        return index\n\n    def ask(self) -&gt; dict | None:\n        \"\"\"Ask the optimizer for a configuration to evaluate.\n\n        Returns:\n            A dictionary with the configuration to evaluate.\n        \"\"\"\n        if not self.ready:\n            raise RuntimeError(\"Call setup() before ask()\")\n\n        if self.finished:\n            return None\n\n        self.ask_count += 1\n        if len(self.evaled) &lt; self.init_random_search_steps:\n            left = set(range(self.N)) - self.evaled - self.failed - self.stoped\n            index = left.pop()\n            fidelity = 1\n        else:\n            index = self._ask()\n            fidelity = self.fidelities[index] + 1\n\n        return {\n            \"config-id\": index,\n            \"config\": self.configs[index],\n            \"fidelity\": fidelity,\n        }\n\n    def tell(self, result: dict | list):\n        \"\"\"Tell the result of a trial to the optimizer.\n\n        Args:\n            result (dict | list[dict]): The result(s) for a trial.\n        \"\"\"\n        if isinstance(result, dict):\n            result = [result]\n        for res in result:\n            self._tell(res)\n\n    def _tell(self, result: dict):\n        self.tell_count += 1\n\n        index = result[\"config-id\"]\n        fidelity = result[\"fidelity\"]\n        # cost = result[\"cost\"]\n        score = result[\"score\"]\n        status = result[\"status\"]\n\n        if not status:\n            self.failed.add(index)\n            return\n\n        if score &gt;= 1.0 - self.scr_thr or fidelity == self.max_fidelity:\n            self.stoped.add(index)\n\n        # update trackers\n        self.curves[index, fidelity - 1] = score\n        self.fidelities[index] = fidelity\n        # self.costs[index] = cost\n        self.history.append(result)\n        self.evaled.add(index)\n        self.eval_count += 1\n\n        if self.patience is not None:\n            assert self.score_history is not None\n            if not np.any(self.score_history[index] &lt; (score - self.tol)):\n                self.stoped.add(index)\n            self.score_history[index][fidelity % self.patience] = score\n\n        self.finished = self._check_is_finished()\n\n    def _check_is_finished(self):\n        \"\"\"Check if there is no more configurations to evaluate.\"\"\"\n        left = set(range(self.N)) - self.evaled - self.failed - self.stoped\n        if not left:\n            return True\n        return False\n\n    def ante(self):\n        \"\"\"Some operations to perform by the tuner before the optimization loop.\n\n        Here: refit the predictors with observed data.\n        \"\"\"\n        if (\n            self.refit\n            and not self.eval_count % self.refit_interval\n            and self.eval_count &gt;= self.refit_init_steps\n        ):\n            self.fit_extra()\n\n    def fit_extra(self):\n        \"\"\"Refit the predictors with observed data.\"\"\"\n        pipeline, curve = self.pipelines, self.curves\n        self.perf_predictor.fit_extra(pipeline, curve)  # type: ignore\n\n    def fit(self, X, curve, cost):\n        \"\"\"Fit the predictors with the given training data.\"\"\"\n        self.perf_predictor.fit(X, curve)  # type: ignore\n        if self.cost_predictor is not None:\n            self.cost_predictor.fit(X, cost)\n\n    def reset_path(self, path: str | None = None):\n        \"\"\"\n        Reset the path of the model.\n\n        Args:\n            path (str, optional): Directory location to store all outputs. If None, a new unique\n                time-stamped directory is chosen.\n        \"\"\"\n        super().reset_path(path)\n        if self.perf_predictor is not None:\n            self.perf_predictor.reset_path(path)\n        if self.cost_predictor is not None:\n            self.cost_predictor.reset_path(path)\n</code></pre>"},{"location":"reference/optimizers/quick/#qtt.optimizers.quick.QuickOptimizer.ante","title":"<code>ante()</code>","text":"<p>Some operations to perform by the tuner before the optimization loop.</p> <p>Here: refit the predictors with observed data.</p> Source code in <code>src/qtt/optimizers/quick.py</code> <pre><code>def ante(self):\n    \"\"\"Some operations to perform by the tuner before the optimization loop.\n\n    Here: refit the predictors with observed data.\n    \"\"\"\n    if (\n        self.refit\n        and not self.eval_count % self.refit_interval\n        and self.eval_count &gt;= self.refit_init_steps\n    ):\n        self.fit_extra()\n</code></pre>"},{"location":"reference/optimizers/quick/#qtt.optimizers.quick.QuickOptimizer.ask","title":"<code>ask()</code>","text":"<p>Ask the optimizer for a configuration to evaluate.</p> <p>Returns:</p> <ul> <li> <code>dict | None</code>           \u2013            <p>A dictionary with the configuration to evaluate.</p> </li> </ul> Source code in <code>src/qtt/optimizers/quick.py</code> <pre><code>def ask(self) -&gt; dict | None:\n    \"\"\"Ask the optimizer for a configuration to evaluate.\n\n    Returns:\n        A dictionary with the configuration to evaluate.\n    \"\"\"\n    if not self.ready:\n        raise RuntimeError(\"Call setup() before ask()\")\n\n    if self.finished:\n        return None\n\n    self.ask_count += 1\n    if len(self.evaled) &lt; self.init_random_search_steps:\n        left = set(range(self.N)) - self.evaled - self.failed - self.stoped\n        index = left.pop()\n        fidelity = 1\n    else:\n        index = self._ask()\n        fidelity = self.fidelities[index] + 1\n\n    return {\n        \"config-id\": index,\n        \"config\": self.configs[index],\n        \"fidelity\": fidelity,\n    }\n</code></pre>"},{"location":"reference/optimizers/quick/#qtt.optimizers.quick.QuickOptimizer.fit","title":"<code>fit(X, curve, cost)</code>","text":"<p>Fit the predictors with the given training data.</p> Source code in <code>src/qtt/optimizers/quick.py</code> <pre><code>def fit(self, X, curve, cost):\n    \"\"\"Fit the predictors with the given training data.\"\"\"\n    self.perf_predictor.fit(X, curve)  # type: ignore\n    if self.cost_predictor is not None:\n        self.cost_predictor.fit(X, cost)\n</code></pre>"},{"location":"reference/optimizers/quick/#qtt.optimizers.quick.QuickOptimizer.fit_extra","title":"<code>fit_extra()</code>","text":"<p>Refit the predictors with observed data.</p> Source code in <code>src/qtt/optimizers/quick.py</code> <pre><code>def fit_extra(self):\n    \"\"\"Refit the predictors with observed data.\"\"\"\n    pipeline, curve = self.pipelines, self.curves\n    self.perf_predictor.fit_extra(pipeline, curve)  # type: ignore\n</code></pre>"},{"location":"reference/optimizers/quick/#qtt.optimizers.quick.QuickOptimizer.load","title":"<code>load(path, reset_paths=True, verbose=True)</code>  <code>classmethod</code>","text":"<p>Loads the model from disk to memory.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>Path to the saved model, minus the file name. This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS). The model file is typically located in os.path.join(path, cls.model_file_name).</p> </li> <li> <code>reset_paths</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to reset the self.path value of the loaded model to be equal to path. It is highly recommended to keep this value as True unless accessing the original self.path value is important. If False, the actual valid path and self.path may differ, leading to strange behaviour and potential exceptions if the model needs to load any other files at a later time.</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to log the location of the loaded file.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>model</code> (              <code>Optimizer</code> )          \u2013            <p>The loaded model object.</p> </li> </ul> Source code in <code>src/qtt/optimizers/optimizer.py</code> <pre><code>@classmethod\ndef load(cls, path: str, reset_paths: bool = True, verbose: bool = True):\n    \"\"\"\n    Loads the model from disk to memory.\n\n    Args:\n        path (str):\n            Path to the saved model, minus the file name.\n            This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS).\n            The model file is typically located in os.path.join(path, cls.model_file_name).\n        reset_paths (bool):\n            Whether to reset the self.path value of the loaded model to be equal to path.\n            It is highly recommended to keep this value as True unless accessing the original self.path value is important.\n            If False, the actual valid path and self.path may differ, leading to strange behaviour and potential exceptions if the model needs to load any other files at a later time.\n        verbose (bool):\n            Whether to log the location of the loaded file.\n\n    Returns:\n        model (Optimizer): The loaded model object.\n    \"\"\"\n    file_path = os.path.join(path, cls.model_file_name)\n    with open(file_path, \"rb\") as f:\n        model = pickle.load(f)\n    if reset_paths:\n        model.path = path\n    if verbose:\n        logger.info(f\"Model loaded from: {file_path}\")\n    return model\n</code></pre>"},{"location":"reference/optimizers/quick/#qtt.optimizers.quick.QuickOptimizer.post","title":"<code>post()</code>","text":"<p>This method is intended for the use with a tuner. It allows to perform some post-processing steps after each tell.</p> Source code in <code>src/qtt/optimizers/optimizer.py</code> <pre><code>def post(self):\n    \"\"\"This method is intended for the use with a tuner.\n    It allows to perform some post-processing steps after each tell.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/optimizers/quick/#qtt.optimizers.quick.QuickOptimizer.reset_path","title":"<code>reset_path(path=None)</code>","text":"<p>Reset the path of the model.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Directory location to store all outputs. If None, a new unique time-stamped directory is chosen.</p> </li> </ul> Source code in <code>src/qtt/optimizers/quick.py</code> <pre><code>def reset_path(self, path: str | None = None):\n    \"\"\"\n    Reset the path of the model.\n\n    Args:\n        path (str, optional): Directory location to store all outputs. If None, a new unique\n            time-stamped directory is chosen.\n    \"\"\"\n    super().reset_path(path)\n    if self.perf_predictor is not None:\n        self.perf_predictor.reset_path(path)\n    if self.cost_predictor is not None:\n        self.cost_predictor.reset_path(path)\n</code></pre>"},{"location":"reference/optimizers/quick/#qtt.optimizers.quick.QuickOptimizer.save","title":"<code>save(path=None, verbose=True)</code>","text":"<p>Saves the model to disk.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Path to the saved model, minus the file name. This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS). If None, self.path is used. The final model file is typically saved to os.path.join(path, self.model_file_name).</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to log the location of the saved file.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>Path to the saved model, minus the file name. Use this value to load the     model from disk via cls.load(path), where cls is the class of the model     object (e.g., model = Model.load(path)).</p> </li> </ul> Source code in <code>src/qtt/optimizers/optimizer.py</code> <pre><code>def save(self, path: str | None = None, verbose: bool = True) -&gt; str:\n    \"\"\"\n    Saves the model to disk.\n\n    Args:\n        path (str): Path to the saved model, minus the file name. This should generally\n            be a directory path ending with a '/' character (or appropriate path separator\n            value depending on OS). If None, self.path is used. The final model file is\n            typically saved to os.path.join(path, self.model_file_name).\n        verbose (bool): Whether to log the location of the saved file.\n\n    Returns:\n        str: Path to the saved model, minus the file name. Use this value to load the\n            model from disk via cls.load(path), where cls is the class of the model\n            object (e.g., model = Model.load(path)).\n    \"\"\"\n    if path is None:\n        path = self.path\n    os.makedirs(path, exist_ok=True)\n    file_path = os.path.join(path, self.model_file_name)\n    with open(file_path, \"wb\") as f:\n        pickle.dump(self, f, protocol=pickle.HIGHEST_PROTOCOL)\n    if verbose:\n        logger.info(f\"Model saved to: {file_path}\")\n    return path\n</code></pre>"},{"location":"reference/optimizers/quick/#qtt.optimizers.quick.QuickOptimizer.setup","title":"<code>setup(n, metafeat=None)</code>","text":"<p>Setup the optimizer for optimization.</p> <p>Create the configurations to evaluate. The configurations are sampled from the configuration space. Optionally, metafeatures of the dataset can be provided.</p> <p>Parameters:</p> <ul> <li> <code>n</code>               (<code>int</code>)           \u2013            <p>The number of configurations to create.</p> </li> <li> <code>metafeat</code>               (<code>Mapping[str, int | float]</code>, default:                   <code>None</code> )           \u2013            <p>The metafeatures of the dataset.</p> </li> </ul> Source code in <code>src/qtt/optimizers/quick.py</code> <pre><code>def setup(\n    self,\n    n: int,\n    metafeat: Mapping[str, int | float] | None = None,\n) -&gt; None:\n    \"\"\"Setup the optimizer for optimization.\n\n    Create the configurations to evaluate. The configurations are sampled from the\n    configuration space. Optionally, metafeatures of the dataset can be provided.\n\n    Args:\n        n (int): The number of configurations to create.\n        metafeat (Mapping[str, int | float], optional): The metafeatures of the dataset.\n    \"\"\"\n    self.N = n\n    self.fidelities = np.zeros(n, dtype=int)\n    self.curves = np.full((n, self.max_fidelity), np.nan, dtype=float)\n    self.costs = None\n    if self.patience is not None:\n        self.score_history = np.zeros((n, self.patience), dtype=float)\n\n    if self.seed is not None:\n        self.cs.seed(self.seed)\n    _configs = self.cs.sample_configuration(n)\n    self.configs = [dict(c) for c in _configs]\n    self.pipelines = pd.DataFrame(self.configs)\n\n    self.metafeat = None\n    if metafeat is not None:\n        self.metafeat = pd.DataFrame([metafeat] * self.N)\n    self.pipelines = pd.concat([self.pipelines, self.metafeat], axis=1)\n\n    self.ready = True\n</code></pre>"},{"location":"reference/optimizers/quick/#qtt.optimizers.quick.QuickOptimizer.setup_pandas","title":"<code>setup_pandas(df, metafeat=None)</code>","text":"<p>Setup the optimizer for optimization.</p> <p>Use an existing DataFrame to create the configurations to evaluate. Optionally, metafeatures of the dataset can be provided.</p> <p>Parameters:</p> <ul> <li> <code>df</code>               (<code>DataFrame</code>)           \u2013            <p>The DataFrame with the configurations to evaluate.</p> </li> <li> <code>metafeat</code>               (<code>Mapping[str, int | float]</code>, default:                   <code>None</code> )           \u2013            <p>The metafeatures of the dataset.</p> </li> </ul> Source code in <code>src/qtt/optimizers/quick.py</code> <pre><code>def setup_pandas(\n    self,\n    df: pd.DataFrame,\n    metafeat: Mapping[str, int | float] | None = None,\n):\n    \"\"\"Setup the optimizer for optimization.\n\n    Use an existing DataFrame to create the configurations to evaluate. Optionally,\n    metafeatures of the dataset can be provided.\n\n    Args:\n        df (pd.DataFrame): The DataFrame with the configurations to evaluate.\n        metafeat (Mapping[str, int | float], optional): The metafeatures of the dataset.\n    \"\"\"\n    self.pipelines = df\n    self.N = len(df)\n    self.fidelities = np.zeros(self.N, dtype=int)\n    self.curves = np.full((self.N, self.max_fidelity), np.nan, dtype=float)\n    self.costs = None\n    if self.patience is not None:\n        self.score_history = np.zeros((self.N, self.patience), dtype=float)\n\n    self.metafeat = None\n    if metafeat is not None:\n        self.metafeat = pd.DataFrame([metafeat] * self.N)\n    self.pipelines = pd.concat([self.pipelines, self.metafeat], axis=1)\n    self.configs = self.pipelines.to_dict(orient=\"records\")\n\n    self.ready = True\n</code></pre>"},{"location":"reference/optimizers/quick/#qtt.optimizers.quick.QuickOptimizer.tell","title":"<code>tell(result)</code>","text":"<p>Tell the result of a trial to the optimizer.</p> <p>Parameters:</p> <ul> <li> <code>result</code>               (<code>dict | list[dict]</code>)           \u2013            <p>The result(s) for a trial.</p> </li> </ul> Source code in <code>src/qtt/optimizers/quick.py</code> <pre><code>def tell(self, result: dict | list):\n    \"\"\"Tell the result of a trial to the optimizer.\n\n    Args:\n        result (dict | list[dict]): The result(s) for a trial.\n    \"\"\"\n    if isinstance(result, dict):\n        result = [result]\n    for res in result:\n        self._tell(res)\n</code></pre>"},{"location":"reference/optimizers/random/","title":"RandomOptimizer","text":""},{"location":"reference/optimizers/random/#qtt.optimizers.rndm.RandomOptimizer","title":"<code>RandomOptimizer</code>","text":"<p>               Bases: <code>Optimizer</code></p> <p>A basic implementation of a random search optimizer.</p> <p>Parameters:</p> <ul> <li> <code>cs</code>               (<code>ConfigurationSpace</code>)           \u2013            <p>Configuration space object.</p> </li> <li> <code>max_fidelity</code>               (<code>int</code>)           \u2013            <p>Maximum fidelity level.</p> </li> <li> <code>n</code>               (<code>int</code>)           \u2013            <p>Number of configurations to sample.</p> </li> <li> <code>patience</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Determines if early stopping should be applied for a single configuration. If the score does not improve for <code>patience</code> steps, the configuration is stopped. Defaults to None.</p> </li> <li> <code>tol</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Tolerance for early stopping. Training stops if the score does not improve by at least <code>tol</code> for <code>patience</code> iterations (if set). Values must be in the range <code>[0.0, inf)</code>. Defaults to 0.0.</p> </li> <li> <code>score_thresh</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Score threshold for early stopping. Defaults to 0.0.</p> </li> <li> <code>path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Path to save the optimizer. Defaults to None.</p> </li> <li> <code>seed</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Random seed. Defaults to None.</p> </li> <li> <code>verbosity</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <p>Verbosity level. Defaults to 2.</p> </li> </ul> Source code in <code>src/qtt/optimizers/rndm.py</code> <pre><code>class RandomOptimizer(Optimizer):\n    \"\"\"A basic implementation of a random search optimizer.\n\n    Args:\n        cs (ConfigurationSpace): Configuration space object.\n        max_fidelity (int): Maximum fidelity level.\n        n (int): Number of configurations to sample.\n        patience (int, optional): Determines if early stopping should be applied for a\n            single configuration. If the score does not improve for `patience` steps,\n            the configuration is stopped. Defaults to None.\n        tol (float, optional): Tolerance for early stopping. Training stops if the score\n            does not improve by at least `tol` for `patience` iterations (if set). Values\n            must be in the range `[0.0, inf)`. Defaults to 0.0.\n        score_thresh (float, optional): Score threshold for early stopping. Defaults to 0.0.\n        path (str, optional): Path to save the optimizer. Defaults to None.\n        seed (int, optional): Random seed. Defaults to None.\n        verbosity (int, optional): Verbosity level. Defaults to 2.\n    \"\"\"\n\n    def __init__(\n        self,\n        cs: ConfigurationSpace,\n        max_fidelity: int,\n        n: int,\n        *,\n        patience: int | None = None,\n        tol: float = 0.0,\n        score_thresh: float = 0.0,\n        #\n        path: str | None = None,\n        seed: int | None = None,\n        verbosity: int = 2,\n    ):\n        super().__init__(path=path)\n        set_logger_verbosity(verbosity, logger)\n        self.verbosity = verbosity\n\n        if seed is not None:\n            fix_random_seeds(seed)\n        self.seed = seed\n\n        self.cs = cs\n        self.max_fidelity = max_fidelity\n        self.candidates = cs.sample_configuration(n)\n        self.N = n\n        self.patience = patience\n        self.tol = tol\n        self.scr_thr = score_thresh\n\n        self.reset()\n\n    def reset(self) -&gt; None:\n        # trackers\n        self.iteration = 0\n        self.ask_count = 0\n        self.tell_count = 0\n        self.init_count = 0\n        self.eval_count = 0\n        self.evaled: set[int] = set()\n        self.stoped: set[int] = set()\n        self.failed: set[int] = set()\n        self.history: list = []\n\n        self.fidelities: np.ndarray = np.zeros(self.N, dtype=int)\n        self.curves: np.ndarray = np.zeros((self.N, self.max_fidelity), dtype=float)\n        self.costs: np.ndarray = np.zeros(self.N, dtype=float)\n\n        if self.patience is not None:\n            self._score_history = np.zeros((self.N, self.patience), dtype=float)\n\n    def ask(self) -&gt; dict | None:\n        left = set(range(self.N)) - self.failed - self.stoped\n        if not left:\n            return None\n        index = random.choice(list(left))\n\n        fidelity = self.fidelities[index] + 1\n\n        return {\n            \"config_id\": index,\n            \"config\": self.candidates[index],\n            \"fidelity\": fidelity,\n        }\n\n    def tell(self, reports: dict | list):\n        if isinstance(reports, dict):\n            reports = [reports]\n        for report in reports:\n            self._tell(report)\n\n    def _tell(self, report: dict):\n        self.tell_count += 1\n\n        index = report[\"config-id\"]\n        fidelity = report[\"fidelity\"]\n        cost = report[\"cost\"]\n        score = report[\"score\"]\n        status = report[\"status\"]\n\n        if not status:\n            self.failed.add(index)\n            return\n\n        # update trackers\n        self.curves[index, fidelity - 1] = score\n        self.fidelities[index] = fidelity\n        self.costs[index] = cost\n        self.history.append(report)\n        self.evaled.add(index)\n        self.eval_count += 1\n\n        if score &gt;= 1.0 - self.scr_thr or fidelity == self.max_fidelity:\n            self.stoped.add(index)\n\n        if self.patience is not None:\n            if not np.any(self._score_history[index] &lt; (score - self.tol)):\n                self.stoped.add(index)\n            self._score_history[index][fidelity % self.patience] = score\n</code></pre>"},{"location":"reference/optimizers/random/#qtt.optimizers.rndm.RandomOptimizer.ante","title":"<code>ante()</code>","text":"<p>This method is intended for the use with a tuner. It allows to perform some pre-processing steps before each ask.</p> Source code in <code>src/qtt/optimizers/optimizer.py</code> <pre><code>def ante(self):\n    \"\"\"This method is intended for the use with a tuner.\n    It allows to perform some pre-processing steps before each ask.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/optimizers/random/#qtt.optimizers.rndm.RandomOptimizer.load","title":"<code>load(path, reset_paths=True, verbose=True)</code>  <code>classmethod</code>","text":"<p>Loads the model from disk to memory.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>Path to the saved model, minus the file name. This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS). The model file is typically located in os.path.join(path, cls.model_file_name).</p> </li> <li> <code>reset_paths</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to reset the self.path value of the loaded model to be equal to path. It is highly recommended to keep this value as True unless accessing the original self.path value is important. If False, the actual valid path and self.path may differ, leading to strange behaviour and potential exceptions if the model needs to load any other files at a later time.</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to log the location of the loaded file.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>model</code> (              <code>Optimizer</code> )          \u2013            <p>The loaded model object.</p> </li> </ul> Source code in <code>src/qtt/optimizers/optimizer.py</code> <pre><code>@classmethod\ndef load(cls, path: str, reset_paths: bool = True, verbose: bool = True):\n    \"\"\"\n    Loads the model from disk to memory.\n\n    Args:\n        path (str):\n            Path to the saved model, minus the file name.\n            This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS).\n            The model file is typically located in os.path.join(path, cls.model_file_name).\n        reset_paths (bool):\n            Whether to reset the self.path value of the loaded model to be equal to path.\n            It is highly recommended to keep this value as True unless accessing the original self.path value is important.\n            If False, the actual valid path and self.path may differ, leading to strange behaviour and potential exceptions if the model needs to load any other files at a later time.\n        verbose (bool):\n            Whether to log the location of the loaded file.\n\n    Returns:\n        model (Optimizer): The loaded model object.\n    \"\"\"\n    file_path = os.path.join(path, cls.model_file_name)\n    with open(file_path, \"rb\") as f:\n        model = pickle.load(f)\n    if reset_paths:\n        model.path = path\n    if verbose:\n        logger.info(f\"Model loaded from: {file_path}\")\n    return model\n</code></pre>"},{"location":"reference/optimizers/random/#qtt.optimizers.rndm.RandomOptimizer.post","title":"<code>post()</code>","text":"<p>This method is intended for the use with a tuner. It allows to perform some post-processing steps after each tell.</p> Source code in <code>src/qtt/optimizers/optimizer.py</code> <pre><code>def post(self):\n    \"\"\"This method is intended for the use with a tuner.\n    It allows to perform some post-processing steps after each tell.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/optimizers/random/#qtt.optimizers.rndm.RandomOptimizer.reset_path","title":"<code>reset_path(path=None)</code>","text":"<p>Reset the path of the model.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Directory location to store all outputs. If None, a new unique time-stamped directory is chosen.</p> </li> </ul> Source code in <code>src/qtt/optimizers/optimizer.py</code> <pre><code>def reset_path(self, path: str | None = None):\n    \"\"\"Reset the path of the model.\n\n    Args:\n        path (str):\n            Directory location to store all outputs.\n            If None, a new unique time-stamped directory is chosen.\n    \"\"\"\n    if path is None:\n        path = setup_outputdir(path=self.name.lower(), path_suffix=self.name)\n    self.path = path\n</code></pre>"},{"location":"reference/optimizers/random/#qtt.optimizers.rndm.RandomOptimizer.save","title":"<code>save(path=None, verbose=True)</code>","text":"<p>Saves the model to disk.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Path to the saved model, minus the file name. This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS). If None, self.path is used. The final model file is typically saved to os.path.join(path, self.model_file_name).</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to log the location of the saved file.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>Path to the saved model, minus the file name. Use this value to load the     model from disk via cls.load(path), where cls is the class of the model     object (e.g., model = Model.load(path)).</p> </li> </ul> Source code in <code>src/qtt/optimizers/optimizer.py</code> <pre><code>def save(self, path: str | None = None, verbose: bool = True) -&gt; str:\n    \"\"\"\n    Saves the model to disk.\n\n    Args:\n        path (str): Path to the saved model, minus the file name. This should generally\n            be a directory path ending with a '/' character (or appropriate path separator\n            value depending on OS). If None, self.path is used. The final model file is\n            typically saved to os.path.join(path, self.model_file_name).\n        verbose (bool): Whether to log the location of the saved file.\n\n    Returns:\n        str: Path to the saved model, minus the file name. Use this value to load the\n            model from disk via cls.load(path), where cls is the class of the model\n            object (e.g., model = Model.load(path)).\n    \"\"\"\n    if path is None:\n        path = self.path\n    os.makedirs(path, exist_ok=True)\n    file_path = os.path.join(path, self.model_file_name)\n    with open(file_path, \"wb\") as f:\n        pickle.dump(self, f, protocol=pickle.HIGHEST_PROTOCOL)\n    if verbose:\n        logger.info(f\"Model saved to: {file_path}\")\n    return path\n</code></pre>"},{"location":"reference/predictors/","title":"Overview","text":"<p>The <code>Predictor</code> class serves as a base class for implementing predictive models within the Quick-Tune-Tool. It provides core functionality for model setup, data handling, training, and persistence (saving/loading), allowing specific predictive models to extend and customize these methods.</p>"},{"location":"reference/predictors/#core-methods","title":"Core Methods","text":"<ul> <li> <p><code>fit</code> and <code>_fit</code>: </p> <ul> <li><code>fit</code>: Public method for training the model. It takes feature data <code>X</code>, target labels <code>y</code>, verbosity level, and any additional arguments.</li> <li><code>_fit</code>: Abstract method where specific model training logic is implemented. Models inheriting from <code>Predictor</code> should override <code>_fit</code> to implement their own fitting procedures.</li> </ul> </li> <li> <p><code>preprocess</code> and <code>_preprocess</code>: </p> <ul> <li><code>preprocess</code>: Wrapper method that calls <code>_preprocess</code> to prepare data for fitting or prediction.</li> <li><code>_preprocess</code>: Abstract method where data transformation logic should be added. Designed to clean and structure input data before model training or inference.    </li> </ul> </li> <li> <p><code>load</code> and <code>save</code>:</p> <ul> <li><code>load</code>: Class method to load a saved model from disk, optionally resetting its path and logging the location.</li> <li><code>save</code>: Saves the current model to disk in a specified path, providing persistence for trained models.</li> </ul> </li> <li> <p><code>predict</code>: Abstract method for generating predictions on new data. Specific predictive models should implement this method based on their inference logic.</p> </li> </ul> <p>This <code>Predictor</code> class offers a foundation for different predictive models, providing essential methods for data handling, training, and saving/loading, with extensibility for custom implementations.</p>"},{"location":"reference/predictors/#available-predictors","title":"Available Predictors","text":"<ul> <li><code>PerfPredictor</code>   Predicts the performance of a configuration on a new dataset.</li> <li><code>CostPredictor</code>   Predicts the cost of training a configuration on a new dataset.</li> </ul>"},{"location":"reference/predictors/cost/","title":"CostPredictor","text":""},{"location":"reference/predictors/cost/#qtt.predictors.cost.CostPredictor","title":"<code>CostPredictor</code>","text":"<p>               Bases: <code>Predictor</code></p> <p>A predictor that predicts the cost of training a configuration on a new dataset.</p> Source code in <code>src/qtt/predictors/cost.py</code> <pre><code>class CostPredictor(Predictor):\n    \"\"\"A predictor that predicts the cost of training a configuration on a new dataset.\"\"\"\n\n    temp_file_name = \"temp_model.pt\"\n\n    def __init__(\n        self,\n        fit_params: dict = {},\n        # refit_params: dict = {},\n        path: str | None = None,\n        seed: int | None = None,\n        verbosity: int = 2,\n    ) -&gt; None:\n        super().__init__(path=path)\n\n        self.fit_params = self._validate_fit_params(fit_params, DEFAULT_FIT_PARAMS)\n        self.seed = seed\n        self.verbose = verbosity\n\n        set_logger_verbosity(verbosity, logger)\n\n    @staticmethod\n    def _validate_fit_params(fit_params, default_params):\n        if not isinstance(fit_params, dict):\n            raise ValueError(\"fit_params must be a dictionary\")\n        for key in fit_params:\n            if key not in default_params:\n                raise ValueError(f\"Unknown fit parameter: {key}\")\n        return {**default_params, **fit_params}\n\n    def _get_model(self):\n        params = {\n            \"in_dim\": [\n                len(self.types_of_features[\"continuous\"]),\n                len(self.types_of_features[\"categorical\"]) + len(self.types_of_features[\"bool\"]),\n            ],\n            \"enc_out_dim\": 16,\n            \"enc_nlayers\": 3,\n            \"enc_hidden_dim\": 128,\n        }\n        model = SimpleMLPRegressor(**params)\n        return model\n\n    def _validate_fit_data(self, X, y):\n        if not isinstance(X, pd.DataFrame):\n            raise ValueError(\"X must be a pandas.DataFrame instance\")\n\n        if not isinstance(y, np.ndarray):\n            raise ValueError(\"y must be a numpy.ndarray instance\")\n\n        if X.shape[0] != y.shape[0]:\n            raise ValueError(\"X and y must have the same number of samples\")\n\n        if y.shape[1] != 1:\n            raise ValueError(\"y must have only one column\")\n\n        if len(set(X.columns)) &lt; len(X.columns):\n            raise ValueError(\n                \"Column names are not unique, please change duplicated column names (in pandas: train_data.rename(columns={'current_name':'new_name'})\"\n            )\n\n    def _validate_predict_data(self, pipeline):\n        if not isinstance(pipeline, pd.DataFrame):\n            raise ValueError(\"pipeline and curve must be pandas.DataFrame instances\")\n\n        if len(set(pipeline.columns)) &lt; len(pipeline.columns):\n            raise ValueError(\n                \"Column names are not unique, please change duplicated column names (in pandas: train_data.rename(columns={'current_name':'new_name'})\"\n            )\n\n    def _preprocess_fit_data(self, df: pd.DataFrame, array: np.ndarray):\n        \"\"\"\n        Process data for fitting the model.\n        \"\"\"\n        self._original_features = list(df.columns)\n\n        df, self.types_of_features, self.features_to_drop = get_types_of_features(df)\n        self._input_features = list(df.columns)\n        continous_features = self.types_of_features[\"continuous\"]\n        categorical_features = self.types_of_features[\"categorical\"]\n        bool_features = self.types_of_features[\"bool\"]\n        self.preprocessor = create_preprocessor(\n            continous_features, categorical_features, bool_features\n        )\n        out = self.preprocessor.fit_transform(df)\n        self._feature_mapping = get_feature_mapping(self.preprocessor)\n        if out.shape[1] != sum(len(v) for v in self._feature_mapping.values()):\n            raise ValueError(\n                \"Error during one-hot encoding data processing for neural network. \"\n                \"Number of columns in df array does not match feature_mapping.\"\n            )\n\n        self.label_scaler = preprocessing.StandardScaler()  # MaxAbsScaler()\n        out_array = self.label_scaler.fit_transform(array)\n\n        return out, out_array\n\n    def _preprocess_predict_data(self, df: pd.DataFrame, fill_missing=True):\n        unexpected_columns = set(df.columns) - set(self._original_features)\n        if len(unexpected_columns) &gt; 0:\n            logger.warning(\n                \"Data contains columns that were not present during fitting: \"\n                f\"{unexpected_columns}\"\n            )\n\n        df = df.drop(columns=self.features_to_drop, errors=\"ignore\")\n\n        missing_columns = set(self._input_features) - set(df.columns)\n        if len(missing_columns) &gt; 0:\n            if fill_missing:\n                logger.warning(\n                    \"Data is missing columns that were present during fitting: \"\n                    f\"{missing_columns}. Trying to fill them with mean values / zeros.\"\n                )\n                for col in missing_columns:\n                    df[col] = None\n            else:\n                raise AssertionError(\n                    \"Data is missing columns that were present during fitting: \"\n                    f\"{missing_columns}. Please fill them with appropriate values.\"\n                )\n        X = self.preprocessor.transform(df)\n        X = np.array(X)\n        X = np.nan_to_num(X)\n        return X\n\n    def _fit_model(\n        self,\n        dataset,\n        learning_rate_init,\n        batch_size,\n        max_iter,\n        early_stop,\n        patience,\n        validation_fraction,\n        tol,\n    ):\n        if self.seed is not None:\n            random.seed(self.seed)\n            np.random.seed(self.seed)\n            torch.manual_seed(self.seed)\n\n        self.device = get_torch_device()\n        _dev = self.device\n        self.model.to(_dev)\n\n        optimizer = torch.optim.AdamW(self.model.parameters(), learning_rate_init)\n\n        patience_counter = 0\n        best_iter = 0\n        best_val_metric = np.inf\n\n        if patience is not None:\n            if early_stop:\n                if validation_fraction &lt; 0 or validation_fraction &gt; 1:\n                    raise AssertionError(\n                        \"validation_fraction must be between 0 and 1 when early_stop is True\"\n                    )\n                logger.info(\n                    f\"Early stopping on validation loss with patience {patience} \"\n                    f\"using {validation_fraction} of the data for validation\"\n                )\n                train_set, val_set = random_split(\n                    dataset=dataset,\n                    lengths=[1 - validation_fraction, validation_fraction],\n                )\n            else:\n                logger.info(f\"Early stopping on training loss with patience {patience}\")\n                train_set = dataset\n                val_set = None\n        else:\n            train_set = dataset\n            val_set = None\n\n        bs = min(batch_size, int(2 ** (3 + np.floor(np.log10(len(train_set))))))\n        train_loader = DataLoader(train_set, batch_size=bs, shuffle=True, drop_last=True)\n        val_loader = None\n        if val_set is not None:\n            bs = min(batch_size, int(2 ** (3 + np.floor(np.log10(len(val_set))))))\n            val_loader = DataLoader(val_set, batch_size=bs)\n\n        cache_dir = os.path.expanduser(\"~/.cache\")\n        cache_dir = os.path.join(cache_dir, \"qtt\", self.name)\n        os.makedirs(cache_dir, exist_ok=True)\n        temp_save_file_path = os.path.join(cache_dir, self.temp_file_name)\n        for it in range(1, max_iter + 1):\n            self.model.train()\n\n            train_loss = []\n            header = f\"TRAIN: ({it}/{max_iter})\"\n            metric_logger = MetricLogger(delimiter=\" \")\n            for batch in metric_logger.log_every(\n                train_loader, len(train_loader) // 10, header, logger\n            ):\n                # forward\n                batch = [item.to(_dev) for item in batch]\n                X, y = batch\n                loss = self.model.train_step(X, y)\n                train_loss.append(loss.item())\n\n                # update\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n                metric_logger.update(loss=loss.item())\n            logger.info(f\"Averaged stats: {str(metric_logger)}\")\n            val_metric = np.mean(train_loss)\n\n            if val_loader is not None:\n                self.model.eval()\n\n                val_loss = []\n                with torch.no_grad():\n                    for batch in val_loader:\n                        batch = [item.to(_dev) for item in batch]\n                        X, y = batch\n                        pred = self.model.predict(X)\n                        loss = torch.nn.functional.l1_loss(pred, y)\n                        val_loss.append(loss.item())\n                val_metric = np.mean(val_loss)\n\n            if patience is not None:\n                if val_metric + tol &lt; best_val_metric:\n                    patience_counter = 0\n                    best_val_metric = val_metric\n                    best_iter = it\n                    torch.save(self.model.state_dict(), temp_save_file_path)\n                else:\n                    patience_counter += 1\n                logger.info(\n                    f\"VAL: {round(val_metric, 4)}  \"\n                    f\"ITER: {it}/{max_iter}  \"\n                    f\"BEST: {round(best_val_metric, 4)} ({best_iter})\"\n                )\n                if patience_counter &gt;= patience:\n                    logger.warning(\n                        \"Early stopping triggered! \"\n                        f\"No improvement in the last {patience} iterations. \"\n                        \"Stopping training...\"\n                    )\n                    break\n\n        if early_stop:\n            self.model.load_state_dict(torch.load(temp_save_file_path, weights_only=True))\n\n    def _fit(\n        self,\n        X: pd.DataFrame,\n        y: ArrayLike,\n        **kwargs,\n    ):\n        if self.is_fit:\n            raise AssertionError(\"Predictor is already fit! Create a new one.\")\n\n        y = np.array(y)\n\n        self._validate_fit_data(X, y)\n        _X, _y = self._preprocess_fit_data(X, y)\n\n        train_dataset = SimpleTorchTabularDataset(_X, _y)\n\n        self.model = self._get_model()\n\n        self._fit_model(train_dataset, **self.fit_params)\n\n        return self\n\n    def _predict(self, **kwargs) -&gt; np.ndarray:\n        \"\"\"Predict the costs of training a configuration on a new dataset.\n\n        Args:\n            X (pd.DataFrame): the configuration to predict.\n        \"\"\"\n        if not self.is_fit or self.model is None:\n            raise AssertionError(\"Model is not fitted yet\")\n\n        X: pd.DataFrame = kwargs.pop(\"X\", None)\n        if X is None:\n            raise ValueError(\"X (pipeline configuration) must be provided\")\n\n        self._validate_predict_data(X)\n        x = self._preprocess_predict_data(X)\n\n        self.model.eval()\n        self.model.to(self.device)\n        x_t = torch.tensor(x, dtype=torch.float32).to(self.device)\n\n        with torch.no_grad():\n            pred = self.model.predict(x_t)\n        out = pred.cpu().squeeze().numpy()\n        return out\n\n    def save(self, path: str | None = None, verbose=True) -&gt; str:\n        # Save on CPU to ensure the model can be loaded on a box without GPU\n        if self.model is not None:\n            self.model = self.model.to(torch.device(\"cpu\"))\n        path = super().save(path, verbose)\n        # Put the model back to the device after the save\n        if self.model is not None:\n            self.model.to(self.device)\n        return path\n\n    @classmethod\n    def load(cls, path: str, reset_paths=True, verbose=True):\n        \"\"\"\n        Loads the model from disk to memory.\n        The loaded model will be on the same device it was trained on (cuda/mps);\n        if the device is it's not available (trained on GPU, deployed on CPU),\n        then `cpu` will be used.\n\n        Parameters\n        ----------\n        path : str\n            Path to the saved model, minus the file name.\n            This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS).\n            The model file is typically located in os.path.join(path, cls.model_file_name).\n        reset_paths : bool, default True\n            Whether to reset the self.path value of the loaded model to be equal to path.\n            It is highly recommended to keep this value as True unless accessing the original self.path value is important.\n            If False, the actual valid path and self.path may differ, leading to strange behaviour and potential exceptions if the model needs to load any other files at a later time.\n        verbose : bool, default True\n            Whether to log the location of the loaded file.\n\n        Returns\n        -------\n        model : cls\n            Loaded model object.\n        \"\"\"\n        model: CostPredictor = super().load(path=path, reset_paths=reset_paths, verbose=verbose)\n        return model\n</code></pre>"},{"location":"reference/predictors/cost/#qtt.predictors.cost.CostPredictor.is_fit","title":"<code>is_fit</code>  <code>property</code>","text":"<p>Returns True if the model has been fit.</p>"},{"location":"reference/predictors/cost/#qtt.predictors.cost.CostPredictor.fit","title":"<code>fit(X, y, **kwargs)</code>","text":"<p>Fit model to predict values in y based on X.</p> <p>Models should not override the <code>fit</code> method, but instead override the <code>_fit</code> method which has the same arguments.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>DataFrame</code>)           \u2013            <p>The training data features.</p> </li> <li> <code>y</code>               (<code>ArrayLike</code>)           \u2013            <p>The training data ground truth labels.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Any additional fit arguments a model supports.</p> </li> </ul> Source code in <code>src/qtt/predictors/predictor.py</code> <pre><code>def fit(self, X: pd.DataFrame, y: ArrayLike, **kwargs):\n    \"\"\"\n    Fit model to predict values in y based on X.\n\n    Models should not override the `fit` method, but instead override the `_fit` method which has the same arguments.\n\n    Args:\n        X (pd.DataFrame):\n            The training data features.\n        y (ArrayLike):\n            The training data ground truth labels.\n        **kwargs :\n            Any additional fit arguments a model supports.\n    \"\"\"\n    out = self._fit(X=X, y=y, **kwargs)\n    if out is None:\n        out = self\n    return out\n</code></pre>"},{"location":"reference/predictors/cost/#qtt.predictors.cost.CostPredictor.load","title":"<code>load(path, reset_paths=True, verbose=True)</code>  <code>classmethod</code>","text":"<p>Loads the model from disk to memory. The loaded model will be on the same device it was trained on (cuda/mps); if the device is it's not available (trained on GPU, deployed on CPU), then <code>cpu</code> will be used.</p>"},{"location":"reference/predictors/cost/#qtt.predictors.cost.CostPredictor.load--parameters","title":"Parameters","text":"<p>path : str     Path to the saved model, minus the file name.     This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS).     The model file is typically located in os.path.join(path, cls.model_file_name). reset_paths : bool, default True     Whether to reset the self.path value of the loaded model to be equal to path.     It is highly recommended to keep this value as True unless accessing the original self.path value is important.     If False, the actual valid path and self.path may differ, leading to strange behaviour and potential exceptions if the model needs to load any other files at a later time. verbose : bool, default True     Whether to log the location of the loaded file.</p>"},{"location":"reference/predictors/cost/#qtt.predictors.cost.CostPredictor.load--returns","title":"Returns","text":"<p>model : cls     Loaded model object.</p> Source code in <code>src/qtt/predictors/cost.py</code> <pre><code>@classmethod\ndef load(cls, path: str, reset_paths=True, verbose=True):\n    \"\"\"\n    Loads the model from disk to memory.\n    The loaded model will be on the same device it was trained on (cuda/mps);\n    if the device is it's not available (trained on GPU, deployed on CPU),\n    then `cpu` will be used.\n\n    Parameters\n    ----------\n    path : str\n        Path to the saved model, minus the file name.\n        This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS).\n        The model file is typically located in os.path.join(path, cls.model_file_name).\n    reset_paths : bool, default True\n        Whether to reset the self.path value of the loaded model to be equal to path.\n        It is highly recommended to keep this value as True unless accessing the original self.path value is important.\n        If False, the actual valid path and self.path may differ, leading to strange behaviour and potential exceptions if the model needs to load any other files at a later time.\n    verbose : bool, default True\n        Whether to log the location of the loaded file.\n\n    Returns\n    -------\n    model : cls\n        Loaded model object.\n    \"\"\"\n    model: CostPredictor = super().load(path=path, reset_paths=reset_paths, verbose=verbose)\n    return model\n</code></pre>"},{"location":"reference/predictors/cost/#qtt.predictors.cost.CostPredictor.predict","title":"<code>predict(**kwargs)</code>","text":"<p>Predicts the output for the given input data.</p> <p>Models should not override the <code>predict</code> method, but instead override the <code>_predict</code> method which has the same arguments.</p> Source code in <code>src/qtt/predictors/predictor.py</code> <pre><code>def predict(self, **kwargs) -&gt; np.ndarray | Tuple[np.ndarray, ...]:\n    \"\"\"\n    Predicts the output for the given input data.\n\n    Models should not override the `predict` method, but instead override the `_predict` method\n    which has the same arguments.\n    \"\"\"\n    return self._predict(**kwargs)\n</code></pre>"},{"location":"reference/predictors/cost/#qtt.predictors.cost.CostPredictor.preprocess","title":"<code>preprocess(**kwargs)</code>","text":"<p>Preprocesses the input data into internal form ready for fitting or inference.</p> Source code in <code>src/qtt/predictors/predictor.py</code> <pre><code>def preprocess(self, **kwargs):\n    \"\"\"\n    Preprocesses the input data into internal form ready for fitting or inference.\n    \"\"\"\n    return self._preprocess(**kwargs)\n</code></pre>"},{"location":"reference/predictors/cost/#qtt.predictors.cost.CostPredictor.reset_path","title":"<code>reset_path(path=None)</code>","text":"<p>Reset the path of the model.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Directory location to store all outputs. If None, a new unique time-stamped directory is chosen.</p> </li> </ul> Source code in <code>src/qtt/predictors/predictor.py</code> <pre><code>def reset_path(self, path: str | None = None):\n    \"\"\"\n    Reset the path of the model.\n\n    Args:\n        path (str, optional):\n            Directory location to store all outputs. If None, a new unique time-stamped directory is chosen.\n    \"\"\"\n    if path is None:\n        path = setup_outputdir(path=self.name.lower())\n    self.path = path\n</code></pre>"},{"location":"reference/predictors/perf/","title":"PerfPredictor","text":""},{"location":"reference/predictors/perf/#qtt.predictors.perf.PerfPredictor","title":"<code>PerfPredictor</code>","text":"<p>               Bases: <code>Predictor</code></p> Source code in <code>src/qtt/predictors/perf.py</code> <pre><code>class PerfPredictor(Predictor):\n    temp_file_name: str = \"temp_model.pt\"\n    train_data_size: int = 4096\n    _fit_data = None\n\n    def __init__(\n        self,\n        fit_params: dict = {},\n        refit_params: dict = {},\n        path: str | None = None,\n        seed: int | None = None,\n        verbosity: int = 2,\n    ) -&gt; None:\n        super().__init__(path=path)\n        self.fit_params = self._validate_fit_params(fit_params, DEFAULT_FIT_PARAMS)\n        self.refit_params = self._validate_fit_params(refit_params, DEFAULT_REFIT_PARAMS)\n        self.seed = seed\n        self.verbosity = verbosity\n\n        set_logger_verbosity(verbosity, logger)\n\n    @staticmethod\n    def _validate_fit_params(fit_params, default_params):\n        \"\"\"\n        Validate hyperparameters for fitting the model.\n\n        Args:\n            fit_params (dict): Hyperparameters for fitting the model.\n            default_params (dict): Default hyperparameters for fitting the model.\n\n        Raises:\n            ValueError: If fit_params is not a dictionary or contains unknown hyperparameters.\n\n        Returns:\n            dict: Validated hyperparameters.\n        \"\"\"\n        if not isinstance(fit_params, dict):\n            raise ValueError(\"fit_params must be a dictionary\")\n        for key in fit_params:\n            if key not in default_params:\n                raise ValueError(f\"Unknown fit parameter: {key}\")\n        return {**default_params, **fit_params}\n\n    def _validate_fit_data(self, pipeline: pd.DataFrame, curve: np.ndarray):\n        \"\"\"\n        Validate data for fitting the model.\n\n        Args:\n            pipeline (pandas.DataFrame): Pipeline data.\n            curve (numpy.ndarray): Curve data.\n\n        Raises:\n            ValueError: If pipeline or curve is not a pandas.DataFrame or numpy.ndarray, or if\n                pipeline and curve have different number of samples, or if column names are not\n                unique.\n\n        Returns:\n            tuple: Validated pipeline and curve data.\n        \"\"\"\n        if not isinstance(pipeline, pd.DataFrame):\n            raise ValueError(\"pipeline must be a pandas.DataFrame instance\")\n\n        if not isinstance(curve, np.ndarray):\n            raise ValueError(\"curve must be a numpy.ndarray instance\")\n\n        if pipeline.shape[0] != curve.shape[0]:\n            raise ValueError(\"pipeline and curve must have the same number of samples\")\n\n        if len(set(pipeline.columns)) &lt; len(pipeline.columns):\n            raise ValueError(\n                \"Column names are not unique, please change duplicated column names (in pandas: train_data.rename(columns={'current_name':'new_name'})\"\n            )\n\n        self._curve_dim = curve.shape[1]\n\n    def _preprocess_fit_data(self, df: pd.DataFrame) -&gt; np.ndarray:\n        \"\"\"\n        Preprocess data for fitting the model.\n\n        Args:\n            df (pandas.DataFrame): Data to preprocess.\n\n        Returns:\n            numpy.ndarray: Preprocessed data.\n        \"\"\"\n        self.original_features = list(df.columns)\n\n        df, self.types_of_features, self.features_to_drop = get_types_of_features(df)\n        self.input_features = list(df.columns)\n\n        self.preprocessor = create_preprocessor(\n            self.types_of_features[\"continuous\"],\n            self.types_of_features[\"categorical\"],\n            self.types_of_features[\"bool\"],\n        )\n        out = self.preprocessor.fit_transform(df)\n        self.feature_mapping = get_feature_mapping(self.preprocessor)\n        if out.shape[1] != sum(len(v) for v in self.feature_mapping.values()):\n            raise ValueError(\n                \"Error during one-hot encoding data processing for neural network. \"\n                \"Number of columns in df array does not match feature_mapping.\"\n            )\n        return np.array(out)\n\n    def _validate_predict_data(self, pipeline, curve):\n        \"\"\"Validate data for prediction. Applies the same steps as _validate_fit_data\n\n        Args:\n            pipeline (pandas.DataFrame): Pipeline data.\n            curve (numpy.ndarray): Curve data.\n\n        Raises:\n            ValueError: If pipeline or curve is not a pandas.DataFrame or numpy.ndarray, or if\n                pipeline and curve have different number of samples, or if column names are not\n                unique.\n\n        Returns:\n            tuple: Validated pipeline and curve data.\n        \"\"\"\n        if not isinstance(pipeline, pd.DataFrame) or not isinstance(curve, np.ndarray):\n            raise ValueError(\"pipeline and curve must be pandas.DataFrame instances\")\n\n        if pipeline.shape[0] != curve.shape[0]:\n            raise ValueError(\"pipeline and curve must have the same number of samples\")\n\n        if len(set(pipeline.columns)) &lt; len(pipeline.columns):\n            raise ValueError(\n                \"Column names are not unique, please change duplicated column names (in pandas: train_data.rename(columns={'current_name':'new_name'})\"\n            )\n\n        if curve.shape[1] != self._curve_dim:\n            raise ValueError(\n                \"curve must have the same number of features as the curve used for fitting\"\n                \" (expected: {self._curve_length}, got: {curve.shape[1]})\"\n            )\n\n    def _preprocess_predict_data(self, df: pd.DataFrame, fill_missing=True):\n        extra_features = list(set(df.columns) - set(self.original_features))\n        if extra_features:\n            logger.warning(\n                f\"Features {extra_features} were not present in training data and are dropped\"\n            )\n            df = df.drop(columns=extra_features, errors=\"ignore\")\n\n        df = df.drop(columns=self.features_to_drop, errors=\"ignore\")\n\n        missing_features = list(set(self.input_features) - set(df.columns))\n        if missing_features:\n            if fill_missing:\n                logger.warning(\n                    f\"Features {missing_features} missing in data. Missing values will be imputed.\"\n                )\n                for col in missing_features:\n                    df[col] = None\n            else:\n                raise AssertionError(f\"Features {missing_features} missing in data.\")\n\n        # process data\n        X = self.preprocessor.transform(df)\n        X = np.array(X)\n        X = np.nan_to_num(X)\n        return X\n\n    def _get_model(self):\n        \"\"\"\n        Return a new instance of the model.\n\n        Returns:\n            SurrogateModel: A new instance of the model.\n        \"\"\"\n        params = {\n            \"in_dim\": [\n                len(self.types_of_features[\"continuous\"]),\n                len(self.types_of_features[\"categorical\"]) + len(self.types_of_features[\"bool\"]),\n            ],\n            \"in_curve_dim\": self._curve_dim,\n        }\n        return SurrogateModel(**params)\n\n    def _train_model(\n        self,\n        dataset,\n        learning_rate_init: float,\n        batch_size: int,\n        max_iter: int,\n        early_stop: bool,\n        patience: int | None,\n        validation_fraction: float,\n        tol: float,\n    ):\n        \"\"\"Train the model on the given dataset.\n\n        Args:\n            dataset: Dataset to train on.\n            learning_rate_init (float): Initial learning rate.\n            batch_size (int): Batch size to use.\n            max_iter (int): Maximum number of iterations to train for.\n            early_stop (bool): If True, stop training when validation loss stops improving.\n            patience (int or None): Number of iterations to wait before stopping training\n                if validation loss does not improve.\n            validation_fraction (float): Fraction of the dataset to use for validation.\n            tol (float): Tolerance for determining when to stop training.\n        \"\"\"\n        if self.seed is not None:\n            random.seed(self.seed)\n            np.random.seed(self.seed)\n            torch.manual_seed(self.seed)\n\n        if self.model is None:\n            raise ValueError(\"Model must be set before training\")\n\n        self.device = get_torch_device()\n        dev = self.device\n        self.model.to(dev)\n\n        optimizer = torch.optim.AdamW(self.model.parameters(), learning_rate_init)\n\n        patience_counter = 0\n        best_iter = 0\n        best_val_metric = np.inf\n\n        if patience is not None:\n            if early_stop:\n                if validation_fraction &lt;= 0 or validation_fraction &gt;= 1:\n                    raise AssertionError(\n                        \"validation_fraction must be between 0 and 1 when early_stop is True\"\n                    )\n                logger.info(\n                    f\"Early stopping on validation loss with patience {patience} \"\n                    f\"using {validation_fraction} of the data for validation\"\n                )\n                train_set, val_set = random_split(\n                    dataset=dataset,\n                    lengths=[1 - validation_fraction, validation_fraction],\n                )\n            else:\n                logger.info(f\"Early stopping on training loss with patience {patience}\")\n                train_set = dataset\n                val_set = None\n        else:\n            train_set = dataset\n            val_set = None\n\n        train_loader = DataLoader(\n            train_set,\n            batch_size=min(batch_size, len(train_set)),\n            shuffle=True,\n            drop_last=True,\n            num_workers=4,\n        )\n        val_loader = None\n        if val_set is not None:\n            val_loader = DataLoader(\n                val_set,\n                batch_size=min(batch_size, len(val_set)),\n                num_workers=4,\n            )\n\n        cache_dir = os.path.join(self.path, \".tmp\")\n        os.makedirs(cache_dir, exist_ok=True)\n        temp_save_file_path = os.path.join(cache_dir, self.temp_file_name)\n        for it in range(1, max_iter + 1):\n            self.model.train()\n\n            train_loss = []\n            header = f\"TRAIN: ({it}/{max_iter})\"\n            metric_logger = MetricLogger(delimiter=\" \")\n            for batch in metric_logger.log_every(\n                train_loader, max(len(train_loader) // 10, 1), header, logger\n            ):\n                # forward\n                batch = (b.to(dev) for b in batch)\n                X, curve, y = batch\n                loss = self.model.train_step(X, curve, y)\n                train_loss.append(loss.item())\n\n                # update\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n                # log\n                metric_logger.update(loss=loss.item())\n                metric_logger.update(lengthscale=self.model.lengthscale)\n                metric_logger.update(noise=self.model.noise)  # type: ignore\n            logger.info(f\"({it}/{max_iter}) Averaged stats: {str(metric_logger)}\")\n            val_metric = np.mean(train_loss)\n\n            if val_loader is not None:\n                self.model.eval()\n\n                val_loss = []\n                with torch.no_grad():\n                    for batch in val_loader:\n                        batch = (b.to(dev) for b in batch)\n                        X, curve, y = batch\n                        pred = self.model.predict(X, curve)\n                        loss = torch.nn.functional.l1_loss(pred.mean, y)\n                        val_loss.append(loss.item())\n                val_metric = np.mean(val_loss)\n\n            if patience is not None:\n                if val_metric + tol &lt; best_val_metric:\n                    patience_counter = 0\n                    best_val_metric = val_metric\n                    best_iter = it\n                    torch.save(self.model.state_dict(), temp_save_file_path)\n                else:\n                    patience_counter += 1\n                logger.info(\n                    f\"VAL: {round(val_metric, 4)}  \"\n                    f\"ITER: {it}/{max_iter}  \"\n                    f\"BEST: {round(best_val_metric, 4)} ({best_iter})\"\n                )\n                if patience_counter &gt;= patience:\n                    logger.log(\n                        15,\n                        \"Stopping training...\"\n                        f\"No improvement in the last {patience} iterations. \",\n                    )\n                    break\n\n        if early_stop:\n            logger.info(\n                f\"Loading best model from iteration {best_iter} with val score {best_val_metric}\"\n            )\n            self.model.load_state_dict(torch.load(temp_save_file_path, weights_only=True))\n\n        if os.path.exists(cache_dir):\n            shutil.rmtree(cache_dir)\n\n        # after training the gp, set its training data\n        # TODO: check if this can be improved\n        self.model.eval()\n        size = min(self.train_data_size, len(dataset))\n        loader = DataLoader(dataset, batch_size=size, shuffle=True)\n        a, b, c = next(iter(loader))\n        a, b, c = a.to(dev), b.to(dev), c.to(dev)\n        self.model.set_train_data(a, b, c)\n\n    def _fit(self, X: pd.DataFrame, y: ArrayLike, **kwargs):\n        if self.is_fit:\n            raise AssertionError(\"Predictor is already fit! Create a new one.\")\n        y = np.array(y)\n\n        self._validate_fit_data(X, y)\n        x = self._preprocess_fit_data(X)\n        train_dataset = CurveRegressionDataset(x, y)\n\n        self.model = self._get_model()\n        self._train_model(train_dataset, **self.fit_params)\n\n        self._model_fit = copy.deepcopy(self.model)\n        self._fit_data = train_dataset\n\n        return self\n\n    def fit_extra(\n        self,\n        X: pd.DataFrame,\n        curve: np.ndarray,\n        fit_params: dict = {},\n    ):\n        if not self.is_fit:\n            raise AssertionError(\"Model is not fitted yet\")\n\n        self._validate_predict_data(X, curve)\n\n        x = self._preprocess_predict_data(X)\n\n        tune_dataset = CurveRegressionDataset(x, curve)\n\n        fit_params = self._validate_fit_params(fit_params, self.refit_params)\n        self._refit_model(tune_dataset, **fit_params)\n\n    def _refit_model(\n        self,\n        dataset,\n        learning_rate_init,\n        batch_size,\n        max_iter,\n        early_stop,\n        patience,\n        tol,\n    ):\n        learning_rate_init = 0.001\n        logger.info(\"Refitting model...\")\n        if self.seed is not None:\n            random.seed(self.seed)\n            np.random.seed(self.seed)\n            torch.manual_seed(self.seed)\n        cache_dir = os.path.join(self.path, \".tmp\")\n        os.makedirs(cache_dir, exist_ok=True)\n        temp_save_file_path = os.path.join(cache_dir, self.temp_file_name)\n\n        num_workers = 4\n        self.device = get_torch_device()\n        dev = self.device\n\n        self.model.to(dev)\n        self.model.eval()\n        torch.save(self.model.state_dict(), temp_save_file_path)\n\n        # initial validation loss\n        loader = DataLoader(\n            dataset,\n            batch_size=min(len(dataset), batch_size),\n            num_workers=num_workers,\n        )\n        val_metric = []\n        for batch in loader:\n            batch = (b.to(dev) for b in batch)\n            X, curve, y = batch\n            pred = self.model.predict(X, curve)\n            loss = torch.nn.functional.l1_loss(pred.mean, y)\n            val_metric.append(loss.item())\n        best_val_metric = np.mean(val_metric)\n        logger.info(f\"Initial validation loss: {best_val_metric}\")\n        patience_counter = 0\n        best_iter = 0\n\n        assert self._fit_data is not None\n        fitting_set = self._fit_data\n        logger.debug(f\"Number of samples in the tuning set: {len(dataset)}\")\n        if len(dataset) &lt; batch_size:\n            logger.warning(\n                f\"Tuning-set size is small ({len(dataset)}).\"\n                \"Using all samples for training + validation. \"\n                f\"Adding samples from training set to reach minimal sample size {batch_size}\"\n            )\n\n        if patience is not None:\n            if early_stop:\n                logger.info(f\"Early stopping on validation loss with patience {patience} \")\n            else:\n                logger.info(f\"Early stopping on training loss with patience {patience}\")\n\n        loader_bs = min(int(2 ** np.floor(np.log2(len(dataset) - 1))), batch_size)\n        train_loader = DataLoader(\n            dataset,\n            batch_size=loader_bs,\n            shuffle=True,\n            drop_last=True,\n            num_workers=num_workers,\n        )\n        val_loader = DataLoader(\n            dataset,\n            batch_size=min(batch_size, len(dataset)),\n            num_workers=num_workers,\n        )\n        extra_loader = None\n        if loader_bs &lt; self.train_data_size:\n            extra_loader = DataLoader(\n                fitting_set,\n                batch_size=batch_size - loader_bs,\n                shuffle=True,\n                num_workers=num_workers,\n            )\n\n        optimizer = torch.optim.AdamW(self.model.parameters(), learning_rate_init)\n        for it in range(1, max_iter + 1):\n            self.model.train()\n\n            train_loss = []\n            header = f\"TRAIN: ({it}/{max_iter})\"\n            metric_logger = MetricLogger(delimiter=\" \")\n            for batch in metric_logger.log_every(train_loader, 1, header, logger):\n                # forward\n                if extra_loader is not None:\n                    b1 = next(iter(extra_loader))\n                    batch = [torch.cat([b1, b2]) for b1, b2 in zip(batch, b1)]\n                batch = (b.to(dev) for b in batch)\n                X, curve, y = batch\n                loss = self.model.train_step(X, curve, y)\n                train_loss.append(loss.item())\n\n                # update\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n                # log\n                metric_logger.update(loss=loss.item())\n                metric_logger.update(lengthscale=self.model.lengthscale)\n                metric_logger.update(noise=self.model.noise)  # type: ignore\n            logger.info(f\"[{it}/{max_iter}]Averaged stats: {str(metric_logger)}\")\n            val_metric = np.mean(train_loss)\n\n            if val_loader is not None:\n                self.model.eval()\n\n                l1 = DataLoader(\n                    dataset,\n                    batch_size=len(dataset),\n                    shuffle=True,\n                )\n                batch = next(iter(l1))\n                if len(dataset) &lt; self.train_data_size:\n                    l2 = DataLoader(\n                        fitting_set,\n                        batch_size=self.train_data_size - loader_bs,\n                        shuffle=True,\n                    )\n                    b2 = next(iter(l2))\n                    batch = [torch.cat([p, q]) for p, q in zip(batch, b2)]\n                batch = (b.to(dev) for b in batch)\n                a, b, c = batch\n                self.model.set_train_data(a, b, c)\n\n                val_loss = []\n                with torch.no_grad():\n                    for batch in val_loader:\n                        batch = (b.to(dev) for b in batch)\n                        X, curve, y = batch\n                        pred = self.model.predict(X, curve)\n                        loss = torch.nn.functional.l1_loss(pred.mean, y)\n                        val_loss.append(loss.item())\n                val_metric = np.mean(val_loss)\n\n            if patience is not None:\n                if val_metric + tol &lt; best_val_metric:\n                    patience_counter = 0\n                    best_val_metric = val_metric\n                    best_iter = it\n                    torch.save(self.model.state_dict(), temp_save_file_path)\n                else:\n                    patience_counter += 1\n                logger.info(\n                    f\"[{it}/{max_iter}]  \"\n                    f\"VAL: {round(val_metric, 4)}  \"\n                    f\"BEST: {round(best_val_metric, 4)} ({best_iter})\",\n                )\n                if patience_counter &gt;= patience:\n                    logger.log(\n                        15,\n                        \"Stopping training...\"\n                        f\"No improvement in the last {patience} iterations. \",\n                    )\n                    break\n\n        if patience:\n            logger.info(f\"Loading best model from iteration {best_iter}\")\n            self.model.load_state_dict(torch.load(temp_save_file_path))\n\n        # remove cache dir\n        if os.path.exists(cache_dir):\n            shutil.rmtree(cache_dir)\n\n        # after training the model, reset GPs training data\n        self.model.eval()\n        if len(dataset) &lt; self.train_data_size:\n            l1 = DataLoader(dataset, batch_size=len(dataset), shuffle=True)\n            l2 = DataLoader(\n                fitting_set,\n                batch_size=self.train_data_size - len(dataset),\n                shuffle=True,\n            )\n            b1 = next(iter(l1))\n            b2 = next(iter(l2))\n            batch = [torch.cat([a, b]) for a, b in zip(b1, b2)]\n            batch = (b.to(dev) for b in batch)\n        else:\n            loader = DataLoader(dataset, batch_size=self.train_data_size, shuffle=True)\n            batch = next(iter(loader))\n            batch = (b.to(dev) for b in batch)\n        a, b, c = batch\n        self.model.set_train_data(a, b, c)\n\n    def _predict(self, **kwargs) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Predict the performance of a configuration `X` on a new dataset `curve`.\n\n        Args:\n            X: the configuration to predict.\n            curve: the dataset to predict on.\n            fill_missing: whether to fill missing values in the dataset.\n\n        Returns:\n            The mean and standard deviation of the predicted performance.\n        \"\"\"\n        if not self.is_fit or self.model is None:\n            raise AssertionError(\"Model is not fitted yet\")\n\n        X: pd.DataFrame = kwargs.pop(\"X\", None)\n        curve: np.ndarray = kwargs.pop(\"curve\", None)\n        fill_missing: bool = kwargs.pop(\"fill_missing\", False)\n\n        if X is None:\n            raise ValueError(\"X (pipeline configuration) is a required argument for this predictor\")\n        if curve is None:\n            raise ValueError(\"curve is a required argument for this predictor\")\n\n        self._validate_predict_data(X, curve)\n        x = self._preprocess_predict_data(X, fill_missing)\n        curve = np.nan_to_num(curve)\n\n        device = self.device\n        self.model.eval()\n        self.model.to(device)\n        x = torch.tensor(x, dtype=torch.float32, device=device)\n        c = torch.tensor(curve, dtype=torch.float32, device=device)\n        mean = np.array([])\n        std = np.array([])\n        with torch.no_grad():\n            bs = 4096  # TODO: make this a parameter\n            for i in range(0, x.shape[0], bs):\n                pred = self.model.predict(x[i : i + bs], c[i : i + bs])\n                mean = np.append(mean, pred.mean.cpu().numpy())\n                std = np.append(std, pred.stddev.cpu().numpy())\n        return mean, std\n\n    def save(self, path: str | None = None, verbose=True) -&gt; str:\n        # Save on CPU to ensure the model can be loaded on a box without GPU\n        if self.model is not None:\n            self.model = self.model.to(torch.device(\"cpu\"))\n        path = super().save(path, verbose)\n        # Put the model back to the device after the save\n        if self.model is not None:\n            self.model.to(self.device)\n        return path\n\n    @classmethod\n    def load(cls, path: str, reset_paths=True, verbose=True) -&gt; \"PerfPredictor\":\n        \"\"\"\n        Loads the model from disk to memory.\n\n        The loaded model will be on the same device it was trained on (e.g., cuda/mps).\n        If the device is unavailable (e.g., trained on GPU but deployed on CPU),\n        the model will be loaded on `cpu`.\n\n        Args:\n            path (str): Path to the saved model, excluding the file name.\n                This should typically be a directory path ending with a '/' character\n                (or appropriate path separator based on OS). The model file is usually\n                located at `os.path.join(path, cls.model_file_name)`.\n            reset_paths (bool, optional): Whether to reset the `self.path` value of the loaded\n                model to be equal to `path`. Defaults to True. Setting this to False may cause\n                inconsistencies between the actual valid path and `self.path`, potentially leading\n                to strange behavior and exceptions if the model needs to load other files later.\n            verbose (bool, optional): Whether to log the location of the loaded file. Defaults to True.\n\n        Returns:\n            cls: The loaded model object.\n        \"\"\"\n        model: PerfPredictor = super().load(path=path, reset_paths=reset_paths, verbose=verbose)\n\n        verbosity = model.verbosity\n        set_logger_verbosity(verbosity, logger)\n        return model\n</code></pre>"},{"location":"reference/predictors/perf/#qtt.predictors.perf.PerfPredictor.is_fit","title":"<code>is_fit</code>  <code>property</code>","text":"<p>Returns True if the model has been fit.</p>"},{"location":"reference/predictors/perf/#qtt.predictors.perf.PerfPredictor.fit","title":"<code>fit(X, y, **kwargs)</code>","text":"<p>Fit model to predict values in y based on X.</p> <p>Models should not override the <code>fit</code> method, but instead override the <code>_fit</code> method which has the same arguments.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>DataFrame</code>)           \u2013            <p>The training data features.</p> </li> <li> <code>y</code>               (<code>ArrayLike</code>)           \u2013            <p>The training data ground truth labels.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Any additional fit arguments a model supports.</p> </li> </ul> Source code in <code>src/qtt/predictors/predictor.py</code> <pre><code>def fit(self, X: pd.DataFrame, y: ArrayLike, **kwargs):\n    \"\"\"\n    Fit model to predict values in y based on X.\n\n    Models should not override the `fit` method, but instead override the `_fit` method which has the same arguments.\n\n    Args:\n        X (pd.DataFrame):\n            The training data features.\n        y (ArrayLike):\n            The training data ground truth labels.\n        **kwargs :\n            Any additional fit arguments a model supports.\n    \"\"\"\n    out = self._fit(X=X, y=y, **kwargs)\n    if out is None:\n        out = self\n    return out\n</code></pre>"},{"location":"reference/predictors/perf/#qtt.predictors.perf.PerfPredictor.load","title":"<code>load(path, reset_paths=True, verbose=True)</code>  <code>classmethod</code>","text":"<p>Loads the model from disk to memory.</p> <p>The loaded model will be on the same device it was trained on (e.g., cuda/mps). If the device is unavailable (e.g., trained on GPU but deployed on CPU), the model will be loaded on <code>cpu</code>.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>Path to the saved model, excluding the file name. This should typically be a directory path ending with a '/' character (or appropriate path separator based on OS). The model file is usually located at <code>os.path.join(path, cls.model_file_name)</code>.</p> </li> <li> <code>reset_paths</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to reset the <code>self.path</code> value of the loaded model to be equal to <code>path</code>. Defaults to True. Setting this to False may cause inconsistencies between the actual valid path and <code>self.path</code>, potentially leading to strange behavior and exceptions if the model needs to load other files later.</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to log the location of the loaded file. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>cls</code> (              <code>PerfPredictor</code> )          \u2013            <p>The loaded model object.</p> </li> </ul> Source code in <code>src/qtt/predictors/perf.py</code> <pre><code>@classmethod\ndef load(cls, path: str, reset_paths=True, verbose=True) -&gt; \"PerfPredictor\":\n    \"\"\"\n    Loads the model from disk to memory.\n\n    The loaded model will be on the same device it was trained on (e.g., cuda/mps).\n    If the device is unavailable (e.g., trained on GPU but deployed on CPU),\n    the model will be loaded on `cpu`.\n\n    Args:\n        path (str): Path to the saved model, excluding the file name.\n            This should typically be a directory path ending with a '/' character\n            (or appropriate path separator based on OS). The model file is usually\n            located at `os.path.join(path, cls.model_file_name)`.\n        reset_paths (bool, optional): Whether to reset the `self.path` value of the loaded\n            model to be equal to `path`. Defaults to True. Setting this to False may cause\n            inconsistencies between the actual valid path and `self.path`, potentially leading\n            to strange behavior and exceptions if the model needs to load other files later.\n        verbose (bool, optional): Whether to log the location of the loaded file. Defaults to True.\n\n    Returns:\n        cls: The loaded model object.\n    \"\"\"\n    model: PerfPredictor = super().load(path=path, reset_paths=reset_paths, verbose=verbose)\n\n    verbosity = model.verbosity\n    set_logger_verbosity(verbosity, logger)\n    return model\n</code></pre>"},{"location":"reference/predictors/perf/#qtt.predictors.perf.PerfPredictor.predict","title":"<code>predict(**kwargs)</code>","text":"<p>Predicts the output for the given input data.</p> <p>Models should not override the <code>predict</code> method, but instead override the <code>_predict</code> method which has the same arguments.</p> Source code in <code>src/qtt/predictors/predictor.py</code> <pre><code>def predict(self, **kwargs) -&gt; np.ndarray | Tuple[np.ndarray, ...]:\n    \"\"\"\n    Predicts the output for the given input data.\n\n    Models should not override the `predict` method, but instead override the `_predict` method\n    which has the same arguments.\n    \"\"\"\n    return self._predict(**kwargs)\n</code></pre>"},{"location":"reference/predictors/perf/#qtt.predictors.perf.PerfPredictor.preprocess","title":"<code>preprocess(**kwargs)</code>","text":"<p>Preprocesses the input data into internal form ready for fitting or inference.</p> Source code in <code>src/qtt/predictors/predictor.py</code> <pre><code>def preprocess(self, **kwargs):\n    \"\"\"\n    Preprocesses the input data into internal form ready for fitting or inference.\n    \"\"\"\n    return self._preprocess(**kwargs)\n</code></pre>"},{"location":"reference/predictors/perf/#qtt.predictors.perf.PerfPredictor.reset_path","title":"<code>reset_path(path=None)</code>","text":"<p>Reset the path of the model.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Directory location to store all outputs. If None, a new unique time-stamped directory is chosen.</p> </li> </ul> Source code in <code>src/qtt/predictors/predictor.py</code> <pre><code>def reset_path(self, path: str | None = None):\n    \"\"\"\n    Reset the path of the model.\n\n    Args:\n        path (str, optional):\n            Directory location to store all outputs. If None, a new unique time-stamped directory is chosen.\n    \"\"\"\n    if path is None:\n        path = setup_outputdir(path=self.name.lower())\n    self.path = path\n</code></pre>"},{"location":"reference/predictors/predictor/","title":"Predictor","text":""},{"location":"reference/predictors/predictor/#qtt.predictors.predictor.Predictor","title":"<code>Predictor</code>","text":"<p>Base class. Implements all low-level functionality.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Directory location to store all outputs. Defaults to None. If None, a new unique time-stamped directory is chosen.</p> </li> <li> <code>name</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Name of the subdirectory inside <code>path</code> where the model will be saved. The final model directory will be <code>os.path.join(path, name)</code>. If None, defaults to the model's class name: <code>self.__class__.__name__</code>.</p> </li> </ul> Source code in <code>src/qtt/predictors/predictor.py</code> <pre><code>class Predictor:\n    \"\"\"Base class. Implements all low-level functionality.\n\n    Args:\n        path (str, optional): Directory location to store all outputs. Defaults to None.\n            If None, a new unique time-stamped directory is chosen.\n        name (str, optional): Name of the subdirectory inside `path` where the model will be saved.\n            The final model directory will be `os.path.join(path, name)`.\n            If None, defaults to the model's class name: `self.__class__.__name__`.\n    \"\"\"\n\n    model_file_name = \"model.pkl\"\n\n    def __init__(\n        self,\n        name: str | None = None,\n        path: str | None = None,\n    ):\n        if name is None:\n            self.name = self.__class__.__name__\n            logger.info(\n                f\"No name was specified for model, defaulting to class name: {self.name}\",\n            )\n        else:\n            self.name = name\n\n        if path is None:\n            self.path: str = setup_outputdir(path=self.name.lower())\n            logger.info(\n                f\"No path was specified for predictor, defaulting to: {self.path}\",\n            )\n        else:\n            self.path = setup_outputdir(path)\n\n        self.model = None\n\n    def reset_path(self, path: str | None = None):\n        \"\"\"\n        Reset the path of the model.\n\n        Args:\n            path (str, optional):\n                Directory location to store all outputs. If None, a new unique time-stamped directory is chosen.\n        \"\"\"\n        if path is None:\n            path = setup_outputdir(path=self.name.lower())\n        self.path = path\n\n    @property\n    def is_fit(self) -&gt; bool:\n        \"\"\"Returns True if the model has been fit.\"\"\"\n        return self.model is not None\n\n    def fit(self, X: pd.DataFrame, y: ArrayLike, **kwargs):\n        \"\"\"\n        Fit model to predict values in y based on X.\n\n        Models should not override the `fit` method, but instead override the `_fit` method which has the same arguments.\n\n        Args:\n            X (pd.DataFrame):\n                The training data features.\n            y (ArrayLike):\n                The training data ground truth labels.\n            **kwargs :\n                Any additional fit arguments a model supports.\n        \"\"\"\n        out = self._fit(X=X, y=y, **kwargs)\n        if out is None:\n            out = self\n        return out\n\n    def _fit(self, X: pd.DataFrame, y: ArrayLike, **kwargs):\n        \"\"\"\n        Fit model to predict values in y based on X.\n\n        Models should override this method with their custom model fit logic.\n        X should not be assumed to be in a state ready for fitting to the inner model, and models may require special preprocessing in this method.\n        It is very important that `X = self.preprocess(X)` is called within `_fit`, or else `predict` and `predict_proba` may not work as intended.\n        It is also important that `_preprocess` is overwritten to properly clean the data.\n        Examples of logic that should be handled by a model include missing value handling, rescaling of features (if neural network), etc.\n        If implementing a new model, it is recommended to refer to existing model implementations and experiment using toy datasets.\n\n        Refer to `fit` method for documentation.\n        \"\"\"\n        raise NotImplementedError\n\n    def _preprocess(self, **kwargs):\n        \"\"\"\n        Data transformation logic should be added here.\n\n        Input data should not be trusted to be in a clean and ideal form, while the output should be in an ideal form for training/inference.\n        Examples of logic that should be added here include missing value handling, rescaling of features (if neural network), etc.\n        If implementing a new model, it is recommended to refer to existing model implementations and experiment using toy datasets.\n\n        In bagged ensembles, preprocessing code that lives in `_preprocess` will be executed on each child model once per inference call.\n        If preprocessing code could produce different output depending on the child model that processes the input data, then it must live here.\n        When in doubt, put preprocessing code here instead of in `_preprocess_nonadaptive`.\n        \"\"\"\n        raise NotImplementedError\n\n    def preprocess(self, **kwargs):\n        \"\"\"\n        Preprocesses the input data into internal form ready for fitting or inference.\n        \"\"\"\n        return self._preprocess(**kwargs)\n\n    @classmethod\n    def load(cls, path: str, reset_paths: bool = True, verbose: bool = True):\n        \"\"\"\n        Loads the model from disk to memory.\n\n        Args:\n            path (str):\n                Path to the saved model, minus the file name.\n                This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS).\n                The model file is typically located in os.path.join(path, cls.model_file_name).\n            reset_paths (bool):\n                Whether to reset the self.path value of the loaded model to be equal to path.\n                It is highly recommended to keep this value as True unless accessing the original self.path value is important.\n                If False, the actual valid path and self.path may differ, leading to strange behaviour and potential exceptions if the model needs to load any other files at a later time.\n            verbose (bool):\n                Whether to log the location of the loaded file.\n\n        Returns:\n            model (Predictor): Loaded model object.\n        \"\"\"\n        file_path = os.path.join(path, cls.model_file_name)\n        with open(file_path, \"rb\") as f:\n            model = pickle.load(f)\n        if reset_paths:\n            model.path = path\n        if verbose:\n            logger.info(f\"Model loaded from: {file_path}\")\n        return model\n\n    def save(self, path: str | None = None, verbose: bool = True) -&gt; str:\n        \"\"\"\n        Saves the model to disk.\n\n        Args:\n            path (str): Path to the saved model, minus the file name.\n                This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS).\n                If None, self.path is used.\n                The final model file is typically saved to os.path.join(path, self.model_file_name).\n            verbose (bool): Whether to log the location of the saved file.\n\n        Returns:\n            path: Path to the saved model, minus the file name. Use this value to load the model from disk via cls.load(path), cls being the class of the model object, such as ```model = PerfPredictor.load(path)```\n        \"\"\"\n        if path is None:\n            path = self.path\n        path = setup_outputdir(path, create_dir=True, warn_if_exist=True)\n        file_path = os.path.join(path, self.model_file_name)\n        with open(file_path, \"wb\") as f:\n            pickle.dump(self, f, protocol=pickle.HIGHEST_PROTOCOL)\n        if verbose:\n            logger.info(f\"Model saved to: {file_path}\")\n        return path\n\n    def predict(self, **kwargs) -&gt; np.ndarray | Tuple[np.ndarray, ...]:\n        \"\"\"\n        Predicts the output for the given input data.\n\n        Models should not override the `predict` method, but instead override the `_predict` method\n        which has the same arguments.\n        \"\"\"\n        return self._predict(**kwargs)\n\n    def _predict(self, **kwargs):\n        \"\"\"\n        Predicts the output for the given input data.\n\n        New predictors should override this method with their custom prediction logic.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"reference/predictors/predictor/#qtt.predictors.predictor.Predictor.is_fit","title":"<code>is_fit</code>  <code>property</code>","text":"<p>Returns True if the model has been fit.</p>"},{"location":"reference/predictors/predictor/#qtt.predictors.predictor.Predictor.fit","title":"<code>fit(X, y, **kwargs)</code>","text":"<p>Fit model to predict values in y based on X.</p> <p>Models should not override the <code>fit</code> method, but instead override the <code>_fit</code> method which has the same arguments.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>DataFrame</code>)           \u2013            <p>The training data features.</p> </li> <li> <code>y</code>               (<code>ArrayLike</code>)           \u2013            <p>The training data ground truth labels.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Any additional fit arguments a model supports.</p> </li> </ul> Source code in <code>src/qtt/predictors/predictor.py</code> <pre><code>def fit(self, X: pd.DataFrame, y: ArrayLike, **kwargs):\n    \"\"\"\n    Fit model to predict values in y based on X.\n\n    Models should not override the `fit` method, but instead override the `_fit` method which has the same arguments.\n\n    Args:\n        X (pd.DataFrame):\n            The training data features.\n        y (ArrayLike):\n            The training data ground truth labels.\n        **kwargs :\n            Any additional fit arguments a model supports.\n    \"\"\"\n    out = self._fit(X=X, y=y, **kwargs)\n    if out is None:\n        out = self\n    return out\n</code></pre>"},{"location":"reference/predictors/predictor/#qtt.predictors.predictor.Predictor.load","title":"<code>load(path, reset_paths=True, verbose=True)</code>  <code>classmethod</code>","text":"<p>Loads the model from disk to memory.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>Path to the saved model, minus the file name. This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS). The model file is typically located in os.path.join(path, cls.model_file_name).</p> </li> <li> <code>reset_paths</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to reset the self.path value of the loaded model to be equal to path. It is highly recommended to keep this value as True unless accessing the original self.path value is important. If False, the actual valid path and self.path may differ, leading to strange behaviour and potential exceptions if the model needs to load any other files at a later time.</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to log the location of the loaded file.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>model</code> (              <code>Predictor</code> )          \u2013            <p>Loaded model object.</p> </li> </ul> Source code in <code>src/qtt/predictors/predictor.py</code> <pre><code>@classmethod\ndef load(cls, path: str, reset_paths: bool = True, verbose: bool = True):\n    \"\"\"\n    Loads the model from disk to memory.\n\n    Args:\n        path (str):\n            Path to the saved model, minus the file name.\n            This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS).\n            The model file is typically located in os.path.join(path, cls.model_file_name).\n        reset_paths (bool):\n            Whether to reset the self.path value of the loaded model to be equal to path.\n            It is highly recommended to keep this value as True unless accessing the original self.path value is important.\n            If False, the actual valid path and self.path may differ, leading to strange behaviour and potential exceptions if the model needs to load any other files at a later time.\n        verbose (bool):\n            Whether to log the location of the loaded file.\n\n    Returns:\n        model (Predictor): Loaded model object.\n    \"\"\"\n    file_path = os.path.join(path, cls.model_file_name)\n    with open(file_path, \"rb\") as f:\n        model = pickle.load(f)\n    if reset_paths:\n        model.path = path\n    if verbose:\n        logger.info(f\"Model loaded from: {file_path}\")\n    return model\n</code></pre>"},{"location":"reference/predictors/predictor/#qtt.predictors.predictor.Predictor.predict","title":"<code>predict(**kwargs)</code>","text":"<p>Predicts the output for the given input data.</p> <p>Models should not override the <code>predict</code> method, but instead override the <code>_predict</code> method which has the same arguments.</p> Source code in <code>src/qtt/predictors/predictor.py</code> <pre><code>def predict(self, **kwargs) -&gt; np.ndarray | Tuple[np.ndarray, ...]:\n    \"\"\"\n    Predicts the output for the given input data.\n\n    Models should not override the `predict` method, but instead override the `_predict` method\n    which has the same arguments.\n    \"\"\"\n    return self._predict(**kwargs)\n</code></pre>"},{"location":"reference/predictors/predictor/#qtt.predictors.predictor.Predictor.preprocess","title":"<code>preprocess(**kwargs)</code>","text":"<p>Preprocesses the input data into internal form ready for fitting or inference.</p> Source code in <code>src/qtt/predictors/predictor.py</code> <pre><code>def preprocess(self, **kwargs):\n    \"\"\"\n    Preprocesses the input data into internal form ready for fitting or inference.\n    \"\"\"\n    return self._preprocess(**kwargs)\n</code></pre>"},{"location":"reference/predictors/predictor/#qtt.predictors.predictor.Predictor.reset_path","title":"<code>reset_path(path=None)</code>","text":"<p>Reset the path of the model.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Directory location to store all outputs. If None, a new unique time-stamped directory is chosen.</p> </li> </ul> Source code in <code>src/qtt/predictors/predictor.py</code> <pre><code>def reset_path(self, path: str | None = None):\n    \"\"\"\n    Reset the path of the model.\n\n    Args:\n        path (str, optional):\n            Directory location to store all outputs. If None, a new unique time-stamped directory is chosen.\n    \"\"\"\n    if path is None:\n        path = setup_outputdir(path=self.name.lower())\n    self.path = path\n</code></pre>"},{"location":"reference/predictors/predictor/#qtt.predictors.predictor.Predictor.save","title":"<code>save(path=None, verbose=True)</code>","text":"<p>Saves the model to disk.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Path to the saved model, minus the file name. This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS). If None, self.path is used. The final model file is typically saved to os.path.join(path, self.model_file_name).</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to log the location of the saved file.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>path</code> (              <code>str</code> )          \u2013            <p>Path to the saved model, minus the file name. Use this value to load the model from disk via cls.load(path), cls being the class of the model object, such as <code>model = PerfPredictor.load(path)</code></p> </li> </ul> Source code in <code>src/qtt/predictors/predictor.py</code> <pre><code>def save(self, path: str | None = None, verbose: bool = True) -&gt; str:\n    \"\"\"\n    Saves the model to disk.\n\n    Args:\n        path (str): Path to the saved model, minus the file name.\n            This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS).\n            If None, self.path is used.\n            The final model file is typically saved to os.path.join(path, self.model_file_name).\n        verbose (bool): Whether to log the location of the saved file.\n\n    Returns:\n        path: Path to the saved model, minus the file name. Use this value to load the model from disk via cls.load(path), cls being the class of the model object, such as ```model = PerfPredictor.load(path)```\n    \"\"\"\n    if path is None:\n        path = self.path\n    path = setup_outputdir(path, create_dir=True, warn_if_exist=True)\n    file_path = os.path.join(path, self.model_file_name)\n    with open(file_path, \"wb\") as f:\n        pickle.dump(self, f, protocol=pickle.HIGHEST_PROTOCOL)\n    if verbose:\n        logger.info(f\"Model saved to: {file_path}\")\n    return path\n</code></pre>"},{"location":"reference/tuners/","title":"Overview","text":"<p>The <code>QuickTuner</code> class is a high-level tuner designed to optimize a given objective function by managing iterative evaluations and coordinating with an <code>Optimizer</code>. It provides comprehensive functionality for logging, result tracking, checkpointing, and handling evaluation budgets.</p>"},{"location":"reference/tuners/#core-methods","title":"Core Methods","text":"<ul> <li> <p><code>run</code>: Executes the optimization process within a specified budget of function evaluations (<code>fevals</code>) or time (<code>time_budget</code>). This method iteratively:</p> <ul> <li>Requests new configurations from the optimizer.</li> <li>Evaluates configurations using the objective function <code>f</code>.</li> <li>Updates the optimizer with evaluation results and logs progress.</li> <li>Saves results based on the specified <code>save_freq</code> setting.</li> </ul> </li> <li> <p><code>save</code> and <code>load</code>: </p> <ul> <li><code>save</code>: Saves the current state of the tuner, including the incumbent, evaluation history, and tuner state.</li> <li><code>load</code>: Loads a previously saved tuner state to resume optimization from where it left off.</li> </ul> </li> </ul>"},{"location":"reference/tuners/#usage-example","title":"Usage Example","text":"<p>The <code>QuickTuner</code> is typically used to optimize an objective function with the support of an optimizer, managing configuration sampling, evaluation, and tracking. It is particularly suited for iterative optimization tasks where tracking the best configuration and logging results are essential.</p>"},{"location":"reference/tuners/image_cls_tuner/","title":"QuickTuner - Image Classification","text":""},{"location":"reference/tuners/image_cls_tuner/#qtt.tuners.image.classification.tuner.QuickImageCLSTuner","title":"<code>QuickImageCLSTuner</code>","text":"<p>               Bases: <code>QuickTuner</code></p> <p>QuickTuner for image classification.</p> <p>Parameters:</p> <ul> <li> <code>data_path</code>               (<code>str</code>)           \u2013            <p>Path to the dataset.</p> </li> <li> <code>path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Path to save the optimizer. Defaults to None.</p> </li> <li> <code>verbosity</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <p>Verbosity level. Defaults to 2.</p> </li> </ul> Source code in <code>src/qtt/tuners/image/classification/tuner.py</code> <pre><code>class QuickImageCLSTuner(QuickTuner):\n    \"\"\"QuickTuner for image classification.\n\n    Args:\n        data_path (str): Path to the dataset.\n        path (str, optional): Path to save the optimizer. Defaults to None.\n        verbosity (int, optional): Verbosity level. Defaults to 2.\n    \"\"\"\n\n    def __init__(\n        self,\n        data_path: str,\n        n: int = 512,\n        path: str | None = None,\n        verbosity: int = 2,\n    ):\n        quick_opt: QuickOptimizer = get_pretrained_optimizer(\"mtlbm/full\")\n\n        trial_info, metafeat = extract_image_dataset_metafeat(data_path)\n        quick_opt.setup(n, metafeat=metafeat)\n\n        self.trial_info = trial_info\n\n        super().__init__(quick_opt, fn, path=path, verbosity=verbosity)\n\n    def run(\n        self,\n        fevals: int | None = None,\n        time_budget: float | None = None,\n        trial_info: dict | None = None,\n    ) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n\n        Args:\n            fevals (int, optional): Number of function evaluations to run. Defaults to None.\n            time_budget (float, optional): Time budget in seconds. Defaults to None.\n            trial_info (dict, optional): Additional information to pass to the objective function. Defaults to None.\n\n        Returns:\n            - np.ndarray: Trajectory of the incumbent scores.\n            - np.ndarray: Runtime of the incumbent evaluations.\n            - np.ndarray: History of all evaluations.\n        \"\"\"\n        if trial_info is not None:\n            self.trial_info = trial_info\n        return super().run(fevals=fevals, time_budget=time_budget, trial_info=self.trial_info)\n</code></pre>"},{"location":"reference/tuners/image_cls_tuner/#qtt.tuners.image.classification.tuner.QuickImageCLSTuner.run","title":"<code>run(fevals=None, time_budget=None, trial_info=None)</code>","text":"<p>Parameters:</p> <ul> <li> <code>fevals</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Number of function evaluations to run. Defaults to None.</p> </li> <li> <code>time_budget</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>Time budget in seconds. Defaults to None.</p> </li> <li> <code>trial_info</code>               (<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>Additional information to pass to the objective function. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[ndarray, ndarray, ndarray]</code>           \u2013            <ul> <li>np.ndarray: Trajectory of the incumbent scores.</li> <li>np.ndarray: Runtime of the incumbent evaluations.</li> <li>np.ndarray: History of all evaluations.</li> </ul> </li> </ul> Source code in <code>src/qtt/tuners/image/classification/tuner.py</code> <pre><code>def run(\n    self,\n    fevals: int | None = None,\n    time_budget: float | None = None,\n    trial_info: dict | None = None,\n) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n\n    Args:\n        fevals (int, optional): Number of function evaluations to run. Defaults to None.\n        time_budget (float, optional): Time budget in seconds. Defaults to None.\n        trial_info (dict, optional): Additional information to pass to the objective function. Defaults to None.\n\n    Returns:\n        - np.ndarray: Trajectory of the incumbent scores.\n        - np.ndarray: Runtime of the incumbent evaluations.\n        - np.ndarray: History of all evaluations.\n    \"\"\"\n    if trial_info is not None:\n        self.trial_info = trial_info\n    return super().run(fevals=fevals, time_budget=time_budget, trial_info=self.trial_info)\n</code></pre>"},{"location":"reference/tuners/quicktuner/","title":"QuickTuner","text":""},{"location":"reference/tuners/quicktuner/#qtt.tuners.quick.QuickTuner","title":"<code>QuickTuner</code>","text":"<p>QuickTuner is a simple tuner that can be used to optimize a given function using a given optimizer.</p> <p>Parameters:</p> <ul> <li> <code>optimizer</code>               (<code>Optimizer</code>)           \u2013            <p>An instance of an Optimizer class.</p> </li> <li> <code>f</code>               (<code>Callable</code>)           \u2013            <p>A function that takes a configuration and returns a score.</p> </li> <li> <code>path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Directory location to store all outputs. Defaults to None. If None, a new unique time-stamped directory is chosen.</p> </li> <li> <code>save_freq</code>               (<code>str</code>, default:                   <code>'step'</code> )           \u2013            <p>Frequency of saving the state of the tuner. Defaults to \"step\". - \"step\": save after each evaluation. - \"incumbent\": save only when the incumbent changes. - None: do not save.</p> </li> <li> <code>verbosity</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <p>Verbosity level of the logger. Defaults to 2.</p> </li> <li> <code>resume</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to resume the tuner from a previous state. Defaults to False.</p> </li> </ul> Source code in <code>src/qtt/tuners/quick.py</code> <pre><code>class QuickTuner:\n    \"\"\"\n    QuickTuner is a simple tuner that can be used to optimize a given function\n    using a given optimizer.\n\n    Args:\n        optimizer (Optimizer): An instance of an Optimizer class.\n        f (Callable): A function that takes a configuration and returns a score.\n        path (str, optional): Directory location to store all outputs. Defaults to None.\n            If None, a new unique time-stamped directory is chosen.\n        save_freq (str, optional): Frequency of saving the state of the tuner. Defaults to \"step\".\n            - \"step\": save after each evaluation.\n            - \"incumbent\": save only when the incumbent changes.\n            - None: do not save.\n        verbosity (int, optional): Verbosity level of the logger. Defaults to 2.\n        resume (bool, optional): Whether to resume the tuner from a previous state. Defaults to False.\n    \"\"\"\n\n    log_to_file: bool = True\n    log_file_name: str = \"quicktuner.log\"\n    log_file_path: str = \"auto\"\n    path_suffix: str = \"tuner\"\n\n    def __init__(\n        self,\n        optimizer: Optimizer,\n        f: Callable,\n        path: str | None = None,\n        save_freq: str | None = \"step\",\n        verbosity: int = 2,\n        resume: bool = False,\n        **kwargs,\n    ):\n        if resume and path is None:\n            raise ValueError(\"Cannot resume without specifying a path.\")\n        self._validate_kwargs(kwargs)\n\n        self.verbosity = verbosity\n        set_logger_verbosity(verbosity, logger)\n\n        self.output_dir = setup_outputdir(path, path_suffix=self.path_suffix)\n        self._setup_log_to_file(self.log_to_file, self.log_file_path)\n\n        if save_freq not in [\"step\", \"incumbent\"] and save_freq is not None:\n            raise ValueError(\"Invalid value for 'save_freq'.\")\n        self.save_freq = save_freq\n\n        self.optimizer = optimizer\n        self.optimizer.reset_path(self.output_dir)\n        self.f = f\n\n        # trackers\n        self.inc_score: float = 0.0\n        self.inc_fidelity: int = -1\n        self.inc_config: dict = {}\n        self.inc_cost: float = 0.0\n        self.inc_info: object = None\n        self.inc_id: int = -1\n        self.traj: list[object] = []\n        self.history: list[object] = []\n        self.runtime: list[object] = []\n\n        self._remaining_fevals = None\n        self._remaining_time = None\n\n        if resume:\n            self.load(os.path.join(self.output_dir, \"qt.json\"))\n\n    def _setup_log_to_file(self, log_to_file: bool, log_file_path: str) -&gt; None:\n        \"\"\"\n        Set up the logging to file.\n\n        Args:\n            log_to_file (bool): Whether to log to file.\n            log_file_path (str | Path): Path to the log file.\n        \"\"\"\n        if not log_to_file:\n            return\n        if log_file_path == \"auto\":\n            log_file_path = os.path.join(self.output_dir, \"logs\", self.log_file_name)\n        log_file_path = os.path.abspath(os.path.normpath(log_file_path))\n        os.makedirs(os.path.dirname(log_file_path), exist_ok=True)\n        add_log_to_file(log_file_path, logger)\n\n    def _is_budget_exhausted(\n        self, fevals: int | None = None, time_budget: float | None = None\n    ) -&gt; bool:\n        \"\"\"\n        Checks if the run budget has been exhausted. Returns whether the run should be terminated.\n        Negative values translate to no budget. If no limit is desired, use None.\n\n        Args:\n            fevals (int, optional): Number of function evaluations. Defaults to None.\n            time_budget (float, optional): Time budget in seconds. Defaults to None.\n\n        Returns:\n            bool: Whether the run should be terminated or continued.\n        \"\"\"\n        if fevals is not None:\n            evals_left = fevals - len(self.traj)\n            if evals_left &lt;= 0:\n                return True\n            logger.info(f\"Evaluations left: {evals_left}\")\n        if time_budget is not None:\n            time_left = time_budget - (time.time() - self.start)\n            if time_left &lt;= 0:\n                return True\n            logger.info(f\"Time left: {time_left:.2f}s\")\n        return False\n\n    def _save_incumbent(self, save: bool = True):\n        \"\"\"\n        Saves the current incumbent configuration and its associated information to a JSON file.\n\n        Args:\n            save (bool, optional): Whether to save the incumbent. Defaults to True.\n        \"\"\"\n        if not self.inc_config or not save:\n            return\n        try:\n            out: dict[str, Any] = {}\n            out[\"config\"] = self.inc_config\n            out[\"score\"] = self.inc_score\n            out[\"cost\"] = self.inc_cost\n            out[\"info\"] = self.inc_info\n            with open(os.path.join(self.output_dir, \"incumbent.json\"), \"w\") as f:\n                json.dump(out, f, indent=2)\n        except Exception as e:\n            logger.error(f\"Failed to save incumbent: {e}\")\n\n    def _save_history(self, save: bool = True):\n        \"\"\"\n        Saves the history of evaluations to a CSV file.\n\n        Args:\n            save (bool, optional): Whether to save the history. Defaults to True.\n        \"\"\"\n        if not self.history or not save:\n            return\n        try:\n            history_path = os.path.join(self.output_dir, \"history.csv\")\n            history_df = pd.DataFrame(self.history)\n            history_df.to_csv(history_path)\n        except Exception as e:\n            logger.warning(f\"History not saved: {e!r}\")\n        finally:\n            logger.info(\"Saved history.\")\n\n    def _log_job_submission(self, trial_info: dict):\n        \"\"\"\n        Logs a message when a job is submitted to the compute backend.\n\n        Args:\n            trial_info (dict): A dictionary containing the trial information.\n        \"\"\"\n        fidelity = trial_info[\"fidelity\"]\n        config_id = trial_info[\"config-id\"]\n        logger.info(\n            f\"INCUMBENT: {self.inc_id}  \"\n            f\"SCORE: {self.inc_score}  \"\n            f\"FIDELITY: {self.inc_fidelity}\",\n        )\n        logger.info(f\"Evaluating configuration {config_id} with fidelity {fidelity}\")\n\n    def _get_state(self):\n        \"\"\"\n        Returns the state of the QuickTuner as a dictionary.\n\n        Returns:\n            A dictionary containing the state of the QuickTuner.\n        \"\"\"\n        state = self.__dict__.copy()\n        state.pop(\"optimizer\")\n        state.pop(\"f\")\n        return state\n\n    def _save_state(self, save: bool = True):\n        \"\"\"\n        Saves the state of the QuickTuner to disk.\n\n        The state of the Tuner is saved as a JSON file to disk, named 'qt.json' in the output\n        directory. If the optimization is interrupted, the state can be loaded from disk to\n        resume the optimization.\n\n        Args:\n            save (bool, optional): Whether to save the state. Defaults to True.\n        \"\"\"\n        if not save:\n            return\n        # Get state\n        state = self._get_state()\n        # Write state to disk\n        try:\n            state_path = os.path.join(self.output_dir, \"qt.json\")\n            with open(state_path, \"wb\") as f:\n                pickle.dump(state, f, protocol=pickle.HIGHEST_PROTOCOL)\n        except Exception as e:\n            logger.warning(f\"State not saved: {e!r}\")\n        finally:\n            logger.info(\"State saved to disk.\")\n        try:\n            opt_path = os.path.join(self.output_dir, \"optimizer\")\n            self.optimizer.save(opt_path)\n        except Exception as e:\n            logger.warning(f\"Optimizer state not saved: {e!r}\")\n\n    def save(self, incumbent: bool = True, history: bool = True, state: bool = True):\n        logger.info(\"Saving current state to disk...\")\n        self._save_incumbent(incumbent)\n        self._save_history(history)\n        self._save_state(state)\n\n    def load(self, path: str):\n        logger.info(f\"Loading state from {path}\")\n        with open(path, \"rb\") as f:\n            state = pickle.load(f)\n        self.__dict__.update(state)\n        self.optimizer = Optimizer.load(os.path.join(self.output_dir, \"optimizer\"))\n\n    def run(\n        self,\n        fevals: int | None = None,\n        time_budget: float | None = None,\n        trial_info: dict | None = None,\n    ) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Run the tuner.\n\n        Args:\n            fevals (int, optional): Number of function evaluations to run. Defaults to None.\n            time_budget (float, optional): Time budget in seconds. Defaults to None.\n            trial_info (dict, optional): Additional information to pass to the objective function. Defaults to None.\n\n        Returns:\n            Tuple[np.ndarray, np.ndarray, np.ndarray]:\n                - trajectory (np.ndarray): Trajectory of the incumbent scores.\n                - runtime (np.ndarray): Runtime of the incumbent evaluations.\n                - history (np.ndarray): History of all evaluations.\n        \"\"\"\n        logger.info(\"Starting QuickTuner Run...\")\n        logger.info(f\"QuickTuneTool will save results to {self.output_dir}\")\n\n        self.start = time.time()\n        while True:\n            self.optimizer.ante()\n\n            # ask for a new configuration\n            trial = self.optimizer.ask()\n            if trial is None:\n                break\n            _trial_info = self._add_trial_info(trial_info)\n\n            self._log_job_submission(trial)\n            result = self.f(trial, trial_info=_trial_info)\n\n            self._log_report(result)\n            self.optimizer.tell(result)\n\n            self.optimizer.post()\n            if self._is_budget_exhausted(fevals, time_budget):\n                logger.info(\"Budget exhausted. Stopping run...\")\n                break\n\n        self._log_end()\n        self.save()\n\n        return (\n            np.array(self.traj),\n            np.array(self.runtime),\n            np.array(self.history, dtype=object),\n        )\n\n    def _update_trackers(self, traj, runtime, history):\n        self.traj.append(traj)\n        self.runtime.append(runtime)\n        self.history.append(history)\n\n    def _log_report(self, reports):\n        if isinstance(reports, dict):\n            reports = [reports]\n\n        inc_changed = False\n        for report in reports:\n            config_id = report[\"config-id\"]\n            score = report[\"score\"]\n            cost = report[\"cost\"]\n            fidelity = report[\"fidelity\"]\n            config = config_to_serializible_dict(report[\"config\"])\n\n            separator = \"-\" * 60\n            logger.info(separator)\n            logger.info(f\"CONFIG ID : {config_id}\")\n            logger.info(f\"FIDELITY  : {fidelity}\")\n            logger.info(f\"SCORE     : {score:.3f}\")\n            logger.info(f\"TIME      : {cost:.3f}\")\n            logger.info(separator)\n\n            if self.inc_score &lt; score:\n                self.inc_score = score\n                self.inc_cost = cost\n                self.inc_fidelity = fidelity\n                self.inc_id = config_id\n                self.inc_config = config\n                self.inc_info = report.get(\"info\")\n                inc_changed = True\n\n            report[\"config\"] = config\n            self._update_trackers(\n                self.inc_score,\n                time.time() - self.start,\n                report,\n            )\n\n        if self.save_freq == \"step\" or (self.save_freq == \"incumbent\" and inc_changed):\n            self.save()\n\n    def _log_end(self):\n        separator = \"=\" * 60\n        logger.info(separator)\n        logger.info(\"RUN COMPLETE - SUMMARY REPORT\")\n        logger.info(separator)\n        logger.info(f\"Best Score        : {self.inc_score:.3f}\")\n        logger.info(f\"Best Cost         : {self.inc_cost:.3f} seconds\")\n        logger.info(f\"Best Config ID    : {self.inc_id}\")\n        logger.info(f\"Best Configuration: {self.inc_config}\")\n        logger.info(separator)\n\n    def _validate_kwargs(self, kwargs: dict) -&gt; None:\n        for key, value in kwargs.items():\n            if hasattr(self, key):\n                setattr(self, key, value)\n            else:\n                logger.warning(f\"Unknown argument: {key}\")\n\n    def _add_trial_info(self, task_info: dict | None) -&gt; dict:\n        out = {} if task_info is None else task_info.copy()\n        out[\"output-dir\"] = self.output_dir\n        out[\"remaining-fevals\"] = self._remaining_fevals\n        out[\"remaining-time\"] = self._remaining_time\n        return out\n\n    def get_incumbent(self):\n        return (\n            self.inc_id,\n            self.inc_config,\n            self.inc_score,\n            self.inc_fidelity,\n            self.inc_cost,\n            self.inc_info,\n        )\n</code></pre>"},{"location":"reference/tuners/quicktuner/#qtt.tuners.quick.QuickTuner.run","title":"<code>run(fevals=None, time_budget=None, trial_info=None)</code>","text":"<p>Run the tuner.</p> <p>Parameters:</p> <ul> <li> <code>fevals</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Number of function evaluations to run. Defaults to None.</p> </li> <li> <code>time_budget</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>Time budget in seconds. Defaults to None.</p> </li> <li> <code>trial_info</code>               (<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>Additional information to pass to the objective function. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[ndarray, ndarray, ndarray]</code>           \u2013            <p>Tuple[np.ndarray, np.ndarray, np.ndarray]:     - trajectory (np.ndarray): Trajectory of the incumbent scores.     - runtime (np.ndarray): Runtime of the incumbent evaluations.     - history (np.ndarray): History of all evaluations.</p> </li> </ul> Source code in <code>src/qtt/tuners/quick.py</code> <pre><code>def run(\n    self,\n    fevals: int | None = None,\n    time_budget: float | None = None,\n    trial_info: dict | None = None,\n) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Run the tuner.\n\n    Args:\n        fevals (int, optional): Number of function evaluations to run. Defaults to None.\n        time_budget (float, optional): Time budget in seconds. Defaults to None.\n        trial_info (dict, optional): Additional information to pass to the objective function. Defaults to None.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray, np.ndarray]:\n            - trajectory (np.ndarray): Trajectory of the incumbent scores.\n            - runtime (np.ndarray): Runtime of the incumbent evaluations.\n            - history (np.ndarray): History of all evaluations.\n    \"\"\"\n    logger.info(\"Starting QuickTuner Run...\")\n    logger.info(f\"QuickTuneTool will save results to {self.output_dir}\")\n\n    self.start = time.time()\n    while True:\n        self.optimizer.ante()\n\n        # ask for a new configuration\n        trial = self.optimizer.ask()\n        if trial is None:\n            break\n        _trial_info = self._add_trial_info(trial_info)\n\n        self._log_job_submission(trial)\n        result = self.f(trial, trial_info=_trial_info)\n\n        self._log_report(result)\n        self.optimizer.tell(result)\n\n        self.optimizer.post()\n        if self._is_budget_exhausted(fevals, time_budget):\n            logger.info(\"Budget exhausted. Stopping run...\")\n            break\n\n    self._log_end()\n    self.save()\n\n    return (\n        np.array(self.traj),\n        np.array(self.runtime),\n        np.array(self.history, dtype=object),\n    )\n</code></pre>"}]}